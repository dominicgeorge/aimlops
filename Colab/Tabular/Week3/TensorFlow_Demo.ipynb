{"cells":[{"cell_type":"markdown","metadata":{"id":"xNe7dVP19Pb8"},"source":["# Advanced Certification Programme in AI and MLOps\n","## A programme by IISc and TalentSprint\n","\n","### Learning Notebook: TensorFlow"]},{"cell_type":"markdown","metadata":{"id":"RdauzVpMUP-W"},"source":["## Learning Objectives"]},{"cell_type":"markdown","metadata":{"id":"-vPeurvKNAEz"},"source":["At the end of the experiment, you will be able to\n","* understand Tensors and their application\n","* define/form Tensors\n","* perform different operations of Tensor using NumPy and Tensorflow library"]},{"cell_type":"markdown","metadata":{"id":"ynL2MOaMNDrg"},"source":["#### Importing required library"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7-9RvK3wugWC"},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf"]},{"cell_type":"markdown","metadata":{"id":"ZtIJ2TbsO91g"},"source":["## Introduction to Tensorflow\n","\n","TensorFlow is an end-to-end open-source platform for machine learning,  specifically deep learning. TensorFlow is a rich system for managing all aspects of a machine learning system; however, we focus on using a particular TensorFlow API to develop and train machine learning models.\n","\n","TensorFlow APIs are arranged hierarchically, with the high-level APIs built on the low-level APIs. Machine learning researchers use low-level APIs to create and explore new machine learning algorithms. We are going to use a high-level API named tf.keras to define and train machine learning models and to make predictions. tf.keras is the TensorFlow variant of the open-source Keras API.\n","\n","Tensorflow's name is directly derived from its core framework: Tensor and all the computations carried out involve Tensor and its operations.TensorFlow was developed by the Google Brain team and first released under the Apache License 2.0 in 2015.\n","\n","[Tensorflow toolkit hierarchy ](https://developers.google.com/machine-learning/crash-course/first-steps-with-tensorflow/toolkit)"]},{"cell_type":"markdown","metadata":{"id":"qxTXOdtXzrrh"},"source":["#### What is a Tensor?\n","\n","A tensor is a generalization of vectors and matrices to potentially higher dimensions, see the Table below. Internally, TensorFlow represents tensors as n-dimensional arrays of base datatypes. Each element in the Tensor has the same data type, and the data type is always known. Simply, tensor, in relation to machine learning, is a generalization of scalars, vectors and, matrices.\n","<br><br>\n","<center>\n","<img src= \"https://cdn.iisc.talentsprint.com/AIandMLOps/Images/Intro_tensor.png\" width=700px/>\n","</center>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"qQUX9GMPFl2U"},"source":["* From the above explanation, you might have understood that the Tensor operations are nothing but matrix operations.\n","\n","* In the following sections, we will see a few commonly used operations of Tensor using both NumPy and TensorFlow Library."]},{"cell_type":"markdown","metadata":{"id":"YGkLfp--LidI"},"source":["### 1. Defining and slicing a 2D/1D-Tensor\n","#### NumPy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u9ar1fwPwmf1"},"outputs":[],"source":["a=np.array([[1,2],[2,3],[6,7]])             # Defining a 2D array in Numpy\n","print(a)                                    # printing the array\n","print('shape = ',a.shape)                   # shape gives counting of number of rows and columns in the forms of tuple\n","print('dimension = ',a.ndim)                # It gives dimension of array , here it is 2D.\n","print('size =',np.size(a))                  # size always gives total number of elements in any array\n","print('lengths = ',len(a))                  # In 2D it gives counting of number of rows in an array\n","print('data Structure type : ',type(a))     # It gives the type of data structure, it is an object of class numpy ndarray .\n","a.dtype                                     # it gives type of data stored in array. For printing the last variable we don't have to write print explicitly."]},{"cell_type":"markdown","source":["#### Slicing: We are going to define a 2D Matrix as given in the image below and apply the slicing operations.\n","<br><br>\n","<center>\n","<img src= \"https://cdn.iisc.talentsprint.com/AIandMLOps/Images/2D_array_slicing.png\" width=600px/>\n","\n","</center>"],"metadata":{"id":"bWURFxA_w4Zy"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"PNFzlFt2QfoI"},"outputs":[],"source":["a2d = np.array([[1, 2, 3, 4], [4, 5, 6, 7], [7, 8, 9, 10]]) ## Creating a 2D array\n","a2d"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8hiI6vCpQryb"},"outputs":[],"source":["a2d[2] # On passing a single integer value inside the square bracket, we get the corresponding indexed row.\n","       # 2nd indexed row is getting sliced here."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2tGDuWBbQvBn"},"outputs":[],"source":["a2d[0] # zeroth row sliced."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rKtiyYnBQzbH"},"outputs":[],"source":["a2d[1,3]  # The first value before coma is for row index and the second value after coma is for column index."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Db2mLx6mQ354"},"outputs":[],"source":["a2d[1][3]  # This is also possible."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rNWmrrTGQ9Gx"},"outputs":[],"source":["a2d[-2,-3] # Using negative index"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L3ZmrbauRETu"},"outputs":[],"source":["a2d[:-1,:-2] # from the very beginning to -2 indexed row (i.e -3 and -2 indexed row) and -3,-4 indexed columns are sliced"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WL3-HuniRJWl"},"outputs":[],"source":["a2d[:2] # from the very beginning to 1 indexed row, i.e 0 and 1 index row sliced. 2 is not included."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IuirfsDkRRXB"},"outputs":[],"source":["a2d[:2, 2:] # ( 0 ,1 ) indexed rows and 2 to last indexed columns are sliced."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OAaf6o7VRbFq"},"outputs":[],"source":["a2d[1, :2] # 1 indexed row and (0,1) indexed columns are sliced ."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P9toXdxKRYG-"},"outputs":[],"source":["a2d[:2, 2] # Explain yourself?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WGBZjVkSRfe1"},"outputs":[],"source":["a2d[:, :1] # All rows and 0 column ( from the very beginning, but 1 not included i.e. zeroth column ) sliced.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tMemWat8Rklm"},"outputs":[],"source":["print('Initial Matrix = ',a2d)\n","a2d[:2, 1:] = 0 # This is an assignment operation, a2d  itself gets changed.\n","print('Matrix after above assigned operations = ',a2d)"]},{"cell_type":"markdown","metadata":{"id":"ftyZwvizMjaP"},"source":["#### TensorFlow\n","**tf.Variable** : There are multiple ways of defining/forming a Tensor in Tensorflow, tf.Variable is one of those. A tf.Variable represents a tensor whose value can be changed by running operations on it. Specific operations allow you to read and modify the values of this tensor. Higher-level libraries like tf.keras use tf.Variable to store model parameters that keep changing/updating with subsequent learning steps.\n","\n","**tf.Constant** : This is another way of creating a Tensor but the tensor made through this cannot be updated but can be called multiple times with only 1 copy in the memory, used in section 10."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G_pY-WC8J5rH"},"outputs":[],"source":["a_tf = tf.Variable([[1,2], [2,3], [6,7]])\n","print(a_tf)\n","print(tf.shape(a_tf))\n","print(tf.rank(a_tf))\n","# print(a_tf.ndim) -->  This operation is not valid for tf.variable object.\n","print(tf.size(a_tf))"]},{"cell_type":"markdown","metadata":{"id":"FQMtT9VYOdI-"},"source":["#### Slicing: Similar to Numpy array slicing."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QSJ1nBOQ9Z1s"},"outputs":[],"source":["a_tf[:,1]"]},{"cell_type":"markdown","metadata":{"id":"r-kOKgPcgK6V"},"source":["#### Note the difference between 1D tensor, 2D row tensor and 2D column tensor, explained with example below.\n","#### NumPy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eCmq5aI0gm_6"},"outputs":[],"source":["V1 = np.array([1, 2, 3])   # This is a 1D tensor. There are no concepts of rows and columns in the 1D tensor.\n","print(V1,'\\n')\n","print(V1.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p06yVFCShM50"},"outputs":[],"source":["V2 = np.array([[1, 2, 3]])    # This is a 2D tensor having 1 row and 3 columns, i.e. 2D row tensor as it contains only one row.\n","print(V2,'\\n')\n","print(V2.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U19HA23rgnPN"},"outputs":[],"source":["V3 = np.array([[1], [2], [3]])  # This is a 2D tensor having 3 rows and 1 column, ie. a 2D column tensor as it contains only one column.\n","print(V3,'\\n')\n","print(V3.shape)"]},{"cell_type":"markdown","metadata":{"id":"c24mn429h452"},"source":["#### TensorFlow"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fgpl2e6Nh_Hy"},"outputs":[],"source":["V1_tf = tf.Variable([1, 2, 3])\n","print(V1_tf)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2eMy_zz-iL2J"},"outputs":[],"source":["V2_tf = tf.Variable([[1, 2, 3]])\n","print(V2_tf)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G8a5oDGuiaLG"},"outputs":[],"source":["V3_tf = tf.Variable([[1], [2], [3]])\n","print(V3_tf)"]},{"cell_type":"markdown","metadata":{"id":"qrF7byEzdr4p"},"source":["### 2. Transpose\n","The new matrix obtained by interchanging the rows and columns of the original matrix is referred to as the transpose of the matrix.\n","#### NumPy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a9wOlq2nduLQ"},"outputs":[],"source":["print(a2d,'\\n')\n","print(a2d.T)"]},{"cell_type":"markdown","metadata":{"id":"k_rdmqffeeJQ"},"source":["#### TensorFlow"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8HXArIhveA7e"},"outputs":[],"source":["a2d_tf = tf.Variable([[1, 2, 3, 4], [4, 5, 6, 7], [7, 8, 9, 10]])     ## Creating a 2D tensor\n","print(a2d_tf)\n","tf.transpose(a2d_tf)"]},{"cell_type":"markdown","metadata":{"id":"ivm4y0dGpgei"},"source":["### 3. Scalar Addition and Multiplication\n","Addition or Multiplication of any higher-order tensor with a scalar quantity(zero-order tensor).\n","#### NumPy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rLPFRvaf4Xu8"},"outputs":[],"source":["print(a)\n","a+2 # 2 is added to each element of the initial matrix a."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JY1H890u4jqH"},"outputs":[],"source":["a*2 # Each element of the initial matrix is multiplied by 2."]},{"cell_type":"markdown","metadata":{"id":"kIRD0ZLF5PNI"},"source":["#### TensorFlow"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zRqRXSLl4YH7"},"outputs":[],"source":["print(a_tf)\n","tf.add(a_tf,2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4dPKvGka4YTg"},"outputs":[],"source":["tf.multiply(a_tf,2)"]},{"cell_type":"markdown","metadata":{"id":"AP_xsfdfiy3a"},"source":["### 4. Addition and Subtraction between tensors\n","When shape matches, simply element-wise addition and subtraction is carried out.\n","#### NumPy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g3eOutTlbSJr"},"outputs":[],"source":["b = np.array([[1,2], [2,3]])\n","c = np.array([[3,4], [5,6]])\n","print('b = ','\\n',b,'\\n','c = ','\\n',c)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JHePyXEFbSX-"},"outputs":[],"source":["print(b+c,'\\n')\n","print(b-c)"]},{"cell_type":"markdown","metadata":{"id":"jDej96KvbRRb"},"source":["#### TensorFlow"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kpNgf3rvZa2Z"},"outputs":[],"source":["b_tf = tf.Variable([[1,2], [2,3]])\n","c_tf = tf.Variable([[3,4], [5,6]])\n","print(b_tf,'\\n',c_tf)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tywFTUvUZbME"},"outputs":[],"source":["tf.add(b_tf, c_tf)       # Addition"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z-qhr0OPabAS"},"outputs":[],"source":["tf.subtract(b_tf, c_tf)   # subtraction"]},{"cell_type":"markdown","source":["Following is a list of commonly used operations. The idea is the same. Each operation requires one or more arguments.\n","\n","    tf.add(a, b)\n","    tf.substract(a, b)\n","    tf.multiply(a, b)\n","    tf.div(a, b)\n","    tf.pow(a, b)\n","    tf.exp(a)\n","    tf.sqrt(a)"],"metadata":{"id":"kbUujJZUo1YW"}},{"cell_type":"markdown","metadata":{"id":"91xEYRpZnWtB"},"source":["#### Concept of Broadcasting\n","It is better understood by going through the examples given below.\n","#### NumPy\n","\n","[Ref.: https://numpy.org/devdocs/user/theory.broadcasting.html ]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ttoVSau6kQUg"},"outputs":[],"source":["M1 = np.array([[1,2,3], [4,5,6]])\n","print(M1)\n","M1.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J7FGP8DxkQj4"},"outputs":[],"source":["M2 = np.array([[8], [9]])\n","print(M2)\n","M2.shape"]},{"cell_type":"markdown","metadata":{"id":"xDi6j-RhniBt"},"source":["Mathematically M1 and M2 cannot be added as the shape doesn't match, but in NumPy, they can be added. M2 has the same number of rows as that of M1. Its column gets replicated so that its shape becomes equal to M1, and then, both are added. This process of replication is broadcasting. M2 gets broadcasted in the direction of the column of M1. We will see the result after replication and sum operation. Broadcasted M2 is not visible, it is a hidden step. See the result below:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1XflAQ3qkQwx"},"outputs":[],"source":["M1 + M2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ubKLjJ09oMGI"},"outputs":[],"source":["# Making another array M3\n","M3 = np.array([[10, 20, 30]])\n","print(M3)\n","M3.shape"]},{"cell_type":"markdown","metadata":{"id":"anTMwjQQoNa1"},"source":["Mathematically M1 and M3 can not be added as the shape doesn't match, but in NumPy, they can be added. M3 has the same number of columns as that of M1. Its rows get replicated so that its shape becomes equal to M1, and then, both are added. This process of replication is broadcasting. M3 gets broadcasted in the direction of the rows of M1. We will see the result after replication and sum operation. Broadcasted M3 is not visible, it is a hidden step. See the result below:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4pQOgvvxoglK"},"outputs":[],"source":["M1 + M3"]},{"cell_type":"markdown","metadata":{"id":"L-wXIU9zcdxr"},"source":["#### TensorFlow\n"," Similar as NumPy.\n","\n"," [Ref.: https://www.tensorflow.org/xla/broadcasting ]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n19AdpclcnFg"},"outputs":[],"source":["M1_tf = tf.Variable([[1,2,3], [4,5,6]])\n","print(M1_tf,'\\n')\n","M2_tf = tf.Variable([[8], [9]])\n","print(M2_tf)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HOUYvim9dFnO"},"outputs":[],"source":["tf.add(M1_tf, M2_tf)"]},{"cell_type":"markdown","metadata":{"id":"RreRdEds5oJ_"},"source":["### 5. Hadamard Product or Element-wise Multiplication\n","If the two Tensors have the same size, operations are carried out elementwise by default.\n","#### NumPy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5hhcBj8x6Kfv"},"outputs":[],"source":["# Using previously defined matrix a by using  Numpy\n","a"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lUO_-LgF7r7u"},"outputs":[],"source":["# Defining another matrix b\n","b = np.array([[2,3], [1,2], [4,5]])\n","b"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aiwerU6u7_uJ"},"outputs":[],"source":["a*b           # Here shape of a  and b matches thus default product is Hadamard multiplication. Note: This is not matrix multiplication."]},{"cell_type":"markdown","metadata":{"id":"GfEjgkI68RKU"},"source":["#### TensorFlow"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jZJenXZc8XHU"},"outputs":[],"source":["a_tf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SWu0-Wzm8XyA"},"outputs":[],"source":["b_tf = tf.Variable([[2,3], [1,2], [4,5]])\n","b_tf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TwYRQO2I8nQM"},"outputs":[],"source":["# Hadamard Multiplication\n","a_tf*b_tf\n","# OR tf.multiply(a_tf,b_tf)"]},{"cell_type":"markdown","metadata":{"id":"TVpqaHzr5jtl"},"source":["### 6. Matrix Multiplication\n","* For multiplying matrix a and matrix b, the number of columns of matrix a and number of rows of matrix b must match.\n","* Note: Matrix Multiplication is not commutative (i.e.AB != BA)"]},{"cell_type":"markdown","source":["<img src= \"https://cdn.iisc.talentsprint.com/AIandMLOps/Images/matrix_multi.png\" width=350px/>\n","\n","</center>"],"metadata":{"id":"TOAXizP8PP1-"}},{"cell_type":"markdown","metadata":{"id":"5rKaS_tBZ8b5"},"source":["#### NumPy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"srzIkcio_dIg"},"outputs":[],"source":["a1 = np.array([[1,2,3], [4,5,6]])\n","print(a1,'\\n','Shape = ', a1.shape)\n","\n","b1 = np.array([[2,1], [3,2], [4,3]])\n","print(b1,'\\n','Shape = ', b1.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8reX_9Xr_dZK"},"outputs":[],"source":["# Matrix multiplication between a1 and b1 is possible as shape matches.\n","np.dot(a1,b1)    # OR\n","a1.dot(b1)"]},{"cell_type":"markdown","metadata":{"id":"kJO9YLfsaAMW"},"source":["#### TensorFlow"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GW14bfECaFHp"},"outputs":[],"source":["a1_tf = tf.Variable([[1,2,3], [4,5,6]])\n","print(a1_tf,'\\n')\n","\n","b1_tf = tf.Variable([[2,1], [3,2], [4,3]])\n","print(b1_tf)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MDHyEVavbGWJ"},"outputs":[],"source":["tf.linalg.matmul(a1_tf, b1_tf)"]},{"cell_type":"markdown","metadata":{"id":"FCc93fa64WSA"},"source":["### 7. Reduction:\n","Calculating the sum across all elements of a tensor along any one or multiple dimensions.\n","#### NumPy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GfcsaOqtmyT4"},"outputs":[],"source":["a                # Using the matrix/tensor defined above using Numpy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uu7riudtmybD"},"outputs":[],"source":["print(a.sum())  # OR\n","print(np.sum(a))"]},{"cell_type":"markdown","metadata":{"id":"rfeANjHNraXN"},"source":["* Summation along any one dimension: Here axis=0 means down the rows and axis=1 means along the columns."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hSnGy0-MrRNg"},"outputs":[],"source":["print(a.sum(axis=0))  # OR\n","print(np.sum(a,axis=0))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_NSRx4JtrVIr"},"outputs":[],"source":["print(a.sum(axis=1))  # OR\n","print(np.sum(a,axis=1))"]},{"cell_type":"markdown","metadata":{"id":"kj2qsYVdsGjg"},"source":["##### Notice the code below and find the difference between the above and below operations."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s4RSKkf2r9kO"},"outputs":[],"source":["print(a.sum(axis=1, keepdims=True))      # OR\n","print(np.sum(a,axis=1, keepdims=True))"]},{"cell_type":"markdown","metadata":{"id":"jacCEzGirHNQ"},"source":["#### TensorFlow"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qmd9pOnjmyjT"},"outputs":[],"source":["a_tf    # Using the tensor a_tf defined using Tensorflow"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U7RBXWZ_s1-S"},"outputs":[],"source":["tf.reduce_sum(a_tf)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g0cnA5fus2MC"},"outputs":[],"source":["tf.reduce_sum(a_tf,0)  # down the rows"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G420vLmzs2XK"},"outputs":[],"source":["tf.reduce_sum(a_tf,1) # along the columns"]},{"cell_type":"markdown","metadata":{"id":"mrNWUO_HveDC"},"source":["### 8. Tensor/Matrix Determinant & Inversion\n","* The matrix inversion is only valid for non-singular matrix i.e. matrix with non zero determinants. All columns of the matrix must be linearly independent.\n","* Inversion is only calculated for the square matrix.\n","#### NumPy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0_vBuCpBtZf5"},"outputs":[],"source":["X = np.array([[2,3], [5,9]])\n","X"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0IM4Kf6QtZyI"},"outputs":[],"source":["np.linalg.det(X)        # Determinant calculation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ffr898zyxdd3"},"outputs":[],"source":["X_inv = np.linalg.inv(X)\n","X_inv"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7DryrhzGxlC_"},"outputs":[],"source":["np.dot(X, X_inv)"]},{"cell_type":"markdown","metadata":{"id":"CYQy5bDgxyRK"},"source":["#### TensorFlow"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W0Cjj6Aax0lC"},"outputs":[],"source":["X_tf = tf.Variable([[2.,3.], [5.,9.]])           # To get inverse, make sure that  tensor has entries as float\n","X_tf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6k5tmZEUyqx9"},"outputs":[],"source":["tf.linalg.det(X_tf)    # Determinant Calculation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kzmfqXYN0WXN"},"outputs":[],"source":["## To get only the final result, add --> .numpy() at the end as given below. Valid everywhere.\n","tf.linalg.det(X_tf).numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zPnSRvrbx03Q"},"outputs":[],"source":["X_tf_inv = tf.linalg.inv(X_tf)\n","X_tf_inv"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jD__pHJxy2Ze"},"outputs":[],"source":["tf.matmul(X_tf, X_tf_inv)"]},{"cell_type":"markdown","source":["### 9. Higher-order tensor\n","A colored image consists of three channels of pixels one for Red color, one for Green color, and one for Blue color. Each channel is a 2D matrix. That means, any colored image is represented by a 3-tensor of pixels composed of three 2D matrices stacked one after another and each 2D matrix is called a channel. Say we have an image of 32 pixels then the shape of the tensor representation of this image is (32,32,3),i.e. three 32X32 matrices are stacked one after another. So this is a tensor of order/rank 3 and it is used for the representation of an image.\n","<br><br>\n","<center>\n","<img src= \"https://cdn.iisc.talentsprint.com/AIandMLOps/Images/Higher_order_tensor.png\" width=480px/>\n","\n","</center>\n","\n"],"metadata":{"id":"5NIJUDElxqdm"}},{"cell_type":"markdown","metadata":{"id":"Cqo8QwnSk-AI"},"source":["Now, say there are 100 such images, then the tensor will be of 4th order and the shape will be (100,32,32,3). So the tensor contains information of 100 images each consists of RGB  channels of 32 by 32 megapixel.\n","#### NumPy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yp4GwRO_kkWQ"},"outputs":[],"source":["np.zeros([3,2,3])        # Making a 3 rank/order tensor filled with zeros."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yd4eXWN2obzr"},"outputs":[],"source":["np.zeros([2,3,2,3])      # Making a 4 rank/order tensor filled with zeros."]},{"cell_type":"markdown","metadata":{"id":"HHQmSvGFqajN"},"source":["#### TensorFlow"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tTxz6fZZnkQD"},"outputs":[],"source":["tf.zeros([3,2,3])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EDcCe5RakZkw"},"outputs":[],"source":["tf.zeros([2,3,2,3])"]},{"cell_type":"markdown","metadata":{"id":"W263Lyklop8B"},"source":["### 11. Converting NumPy Tensor into TensorFlow Tensor and vice-versa :"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4qzVJJoeo4FJ"},"outputs":[],"source":["x = tf.constant([[1, 2, 3], [4, 5, 6]])\n","x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LxF0-D75pD8z"},"outputs":[],"source":["x_np = x.numpy()\n","x_np"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I_3gsLeQpXdT"},"outputs":[],"source":["x_tf = tf.convert_to_tensor(x_np)\n","x_tf"]},{"cell_type":"markdown","metadata":{"id":"h6AeoqU2UQAw"},"source":["**Comparison --> Tensor in NumPy and TensorFlow** : We have carried out different operations of Tensor using both NumPy and TensorFlow libraries. In general, the Tensors defined in NumPy are called Nd-Arrays whereas in TensorFlow they are called Tensors.\n","* Do NumPy arrays differ from Tensors?\n","\n","There is no difference between Tensors defined through both the libraries apart from the syntactical difference (that we have seen in the above operations) but a Tensor is a more suitable choice if we are going to use GPUs/TPUs as it can reside in accelerators memory. This is the main reasoning behind the application of TensorFlow in deep learning."]},{"cell_type":"markdown","metadata":{"id":"O1kyY1mt6vdv"},"source":["##### Many other operations can be explored at the link :>  https://www.tensorflow.org/guide/tensor"]},{"cell_type":"markdown","metadata":{"id":"6GEbdzc3UQAx"},"source":["**Q.1. Which of the following may be the shape of a tensor representing 10 color images of size 64 X 64?**\n","\n","a) (10,3,64,64)\n","\n","b) (10,64,3)\n","\n","c) (64,10,3,64)\n","\n","d)(64,64,10,3)\n","\n","Answer: a)"]},{"cell_type":"markdown","metadata":{"id":"5k3n-IXOUQAy"},"source":["**Q.2. Two tensors having different shapes**\n","\n","a) can not be added.\n","\n","b) can be added mathematically.\n","\n","c) can be added in NumPy but not in TensorFlow.\n","\n","d) can be added in both NumPy and TensorFlow.\n","\n","Answer : d)"]},{"cell_type":"markdown","metadata":{"id":"Wx7r9ekIUQAz"},"source":["**Q.3. Which of the following are not true about matrix multiplication?**\n","\n","a) Hadamard and Matrix Multiplication are the same operations.\n","\n","b) Matrix Multiplication can only be done between matrices of the same shape.\n","\n","c) Element-wise multiplication is the same as that of Matrix Multiplication.\n","\n","d) Number of columns of the first matrix and number of rows of the second matrix must match for matrix multiplication.\n","\n","Answer: a),b) and c)"]},{"cell_type":"markdown","metadata":{"id":"hXLhCUj0UQA0"},"source":["**Q.4. The concept of broadcasting is related to**\n","\n","a) Inverse of a matrix.\n","\n","b) Multiplication of matrix.\n","\n","c) Addition of matrix.\n","\n","d) Subtraction of Matrix.\n","\n","Answer: Both c) and d)\n"]},{"cell_type":"markdown","source":["---\n","---\n"],"metadata":{"id":"V2llZ5BAqiT5"}}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"}},"nbformat":4,"nbformat_minor":0}