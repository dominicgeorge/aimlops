{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Certification Programme in AI and MLOps\n",
        "## A programme by IISc and TalentSprint\n",
        "### Assignment 3: Residuals, Filters, CNN"
      ],
      "metadata": {
        "id": "PprHHREe8n5R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning Objectives:\n",
        "\n",
        "At the end of the experiment, you will be able to:\n",
        "1.   Build important blocks for modern CNNs\n",
        "      - Residual connections\n",
        "      - Batch normalisation\n",
        "      - Depthwise separable convolution\n",
        "2.   Interpret what CNNs learn\n",
        "      - Visualising activations\n",
        "      - Visualising filters\n",
        "      - Visualising heatmaps\n"
      ],
      "metadata": {
        "id": "4bpgBd9QYP0_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Important building blocks for modern CNNs\n",
        "\n",
        "Here we will study about 3 important building blocks:\n",
        "* Residual connection\n",
        "* Batch normalization\n",
        "* Depthwise separable convolution\n",
        "\n"
      ],
      "metadata": {
        "id": "gqTI0D3bYjsJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Residual connections\n",
        "\n",
        "Why do we need them?\n",
        "\n",
        "* CNNs can become extremely deep.\n",
        "* Prone to the vanishing gradient problem\n",
        "\n",
        "Solution:\n",
        "* allow gradients to flow through another shortcut\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Q7q-wwT5Y7Oh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![picture](https://drive.google.com/uc?export=view&id=1gOwZPYnxfCGSsevLCzc7_41apUrkJKI5)"
      ],
      "metadata": {
        "id": "Ryb9kj388SSs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNLA8HiKxQhc"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxGV0fjVvyxy"
      },
      "outputs": [],
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ml321kZyvyxz"
      },
      "outputs": [],
      "source": [
        "#@title Please enter your password (your registered phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "YZ3dzQ3pvyxz"
      },
      "outputs": [],
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "\n",
        "ipython = get_ipython()\n",
        "\n",
        "notebook= \"M2_AST_03_Modern_CNN_Architectures_C\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "#  ipython.magic(\"sx pip3 install torch\")\n",
        "\n",
        "    ipython.magic(\"sx wget https://cdn.iisc.talentsprint.com/AIandMLOps/Datasets/cats_vs_dogs_small.zip\")\n",
        "    ipython.magic(\"sx unzip '/content/cats_vs_dogs_small.zip'\")\n",
        "    ipython.magic(\"sx gdown https://drive.google.com/uc?id=1QrATuMoM1o0UKkhc_R4jX2ZI01yOYU-n\")\n",
        "\n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "\n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "\n",
        "    elif getAnswer() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional,\n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id,\n",
        "              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://aimlops-iisc.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "\n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional\n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "\n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "\n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer():\n",
        "  try:\n",
        "    if not Answer:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Answer\n",
        "  except NameError:\n",
        "    print (\"Please answer Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getId():\n",
        "  try:\n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup\n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup()\n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import libraries"
      ],
      "metadata": {
        "id": "7EBDiTOcWlv0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np  # Import the numpy library, commonly used for numerical operations on arrays and matrices.\n",
        "import pandas as pd  # Import the pandas library, useful for data manipulation and analysis with DataFrames.\n",
        "import matplotlib.pyplot as plt  # Import the pyplot module from matplotlib, mainly used for plotting data visualizations.\n",
        "\n",
        "from tensorflow import keras  # Import the core TensorFlow Keras API, used for building and training neural networks.\n",
        "from tensorflow.keras import layers  # Import layers from Keras, which provide building blocks for creating neural network architectures.\n",
        "from tensorflow.keras.utils import plot_model  # Import the plot_model utility from Keras to visualize the architecture of neural networks."
      ],
      "metadata": {
        "id": "GRCDcQ_VBeAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Build model with residual connection**"
      ],
      "metadata": {
        "id": "Jjbyoe08y6YU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape=(32, 32, 3), name=\"input\")  # Define the input layer with shape 32x32x3 (e.g., a 32x32 RGB image).\n",
        "\n",
        "x = layers.Conv2D(32, 3, activation=\"relu\", padding=\"same\", name=\"C1\")(inputs)\n",
        "# Apply a 2D convolutional layer (C1) with 32 filters of size 3x3.\n",
        "# Activation function is ReLU; padding is 'same' to keep output size the same as input.\n",
        "\n",
        "residual = x  # Store the output of the first convolution layer (C1) as the residual for a skip connection.\n",
        "\n",
        "x = layers.Conv2D(32, 3, activation=\"relu\", padding=\"same\", name=\"C2a\")(x)\n",
        "# Apply another 2D convolutional layer (C2a) with the same parameters as C1, transforming the feature maps further.\n",
        "\n",
        "x = layers.add([x, residual])  # Add the output of C2a to the residual connection, creating a shortcut connection (skip connection).\n",
        "# Both x and residual have the same shape due to the 'same' padding.\n",
        "\n",
        "x = layers.Conv2D(32, 3)(x)  # Apply a final 2D convolution layer with 32 filters and a 3x3 kernel, without specifying activation or padding.\n",
        "\n",
        "model1 = keras.Model(inputs=inputs, outputs=x)  # Define the Keras Model, specifying the inputs and the final output layer.\n",
        "\n",
        "plot_model(model1, show_shapes=True)  # Plot the model architecture with layer shapes displayed."
      ],
      "metadata": {
        "id": "nYkBqRTP_TJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1.summary()  # Print a summary of the model's architecture, showing each layer's details, output shapes, and the total number of parameters."
      ],
      "metadata": {
        "id": "be3LFj5gDg3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Residual branch may contain 1 layer to make sure addition is possible, i.e. accomodate sizes."
      ],
      "metadata": {
        "id": "De25vNGd-0Ae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build model with a residual connection that includes an additional layer in the residual branch\n",
        "\n",
        "inputs = keras.Input(shape=(32, 32, 3), name=\"input\")  # Define the input layer with shape 32x32x3 for an RGB image.\n",
        "x = layers.Conv2D(32, 3, activation=\"relu\", name=\"C1\")(inputs)\n",
        "# Apply a 2D convolutional layer (C1) with 32 filters and a 3x3 kernel.\n",
        "# The activation function is ReLU. This is the first layer of the main path.\n",
        "\n",
        "residual = x  # Store the output of C1 as the residual connection (skip connection) for later addition.\n",
        "\n",
        "x = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\", name=\"C2l1\")(x)\n",
        "# Apply a 2D convolutional layer (C2l1) with 64 filters and a 3x3 kernel, padding set to 'same' to keep dimensions consistent.\n",
        "\n",
        "x = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\", name=\"C2l2\")(x)\n",
        "# Apply another 2D convolutional layer (C2l2) with the same parameters as C2l1, further transforming the feature maps.\n",
        "\n",
        "residual = layers.Conv2D(64, 1, name=\"C2b\")(residual)\n",
        "# Apply a 1x1 convolution (C2b) to the residual to match the shape of `x`.\n",
        "# The 1x1 filter adjusts the channel dimension to 64 without changing the spatial size.\n",
        "\n",
        "x = layers.add([x, residual])\n",
        "# Add the transformed residual branch to `x` to create a shortcut connection (skip connection).\n",
        "# `x` and `residual` now have the same shape due to the adjustments made.\n",
        "\n",
        "model2 = keras.Model(inputs=inputs, outputs=x)  # Define the model with specified inputs and outputs."
      ],
      "metadata": {
        "id": "Rjv_A_9PY6A3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model(model2, show_shapes=True)  # Visualize the model architecture, displaying each layer's shape and connections."
      ],
      "metadata": {
        "id": "9M3gPZRmksDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2.summary()"
      ],
      "metadata": {
        "id": "gzTDlBrsDrws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important**: Add layers of **same** shape !"
      ],
      "metadata": {
        "id": "HkL86M3HB7Gs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q: Can we add layers of shapes (30,30,64) and (30,30,32)?\n",
        "\n",
        "A: No\n",
        "\n",
        "Q: But what happens if you have a pooling layer in between?\n",
        "\n",
        "A: Spatial Dimension reduction due to stride\n",
        "\n",
        "Solution: Use Strides in the  Conv layer in the skip connection."
      ],
      "metadata": {
        "id": "JyQydIMuB8Vw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model with residual connections and a max pool layer in between"
      ],
      "metadata": {
        "id": "UMfQcikJ8tqh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model with residual connections and a max pooling layer in between\n",
        "\n",
        "inputs = keras.Input(shape=(32, 32, 3))  # Define the input layer with shape 32x32x3 for an RGB image.\n",
        "\n",
        "x = layers.Conv2D(32, 3, activation=\"relu\")(inputs)\n",
        "# Apply a 2D convolutional layer with 32 filters and a 3x3 kernel, using ReLU activation.\n",
        "# This is the first layer of the main path.\n",
        "\n",
        "residual = x  # Store the output of the initial convolutional layer as the residual for the skip connection.\n",
        "\n",
        "x = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)\n",
        "# Apply another convolutional layer with 64 filters and a 3x3 kernel.\n",
        "# Padding is set to 'same' to maintain spatial dimensions.\n",
        "\n",
        "x = layers.MaxPooling2D(2, padding=\"same\")(x)\n",
        "# Apply a max pooling layer with a 2x2 pool size, reducing the spatial dimensions by half.\n",
        "# The 'same' padding ensures dimensions remain consistent at the boundary.\n",
        "\n",
        "residual = layers.Conv2D(64, 1, strides=2)(residual)\n",
        "# Apply a 1x1 convolution to the residual connection with a stride of 2, matching the downsampling done by max pooling.\n",
        "# This adjusts both the depth (to 64 channels) and the spatial dimensions of the residual branch.\n",
        "\n",
        "x = layers.add([x, residual])\n",
        "# Add the transformed residual connection to the main path, forming a shortcut connection.\n",
        "# Both `x` and `residual` now have the same shape due to adjustments.\n",
        "\n",
        "model3 = keras.Model(inputs=inputs, outputs=x)  # Define the model with the specified input and final output layer.\n",
        "\n",
        "plot_model(model3, show_shapes=True)  # Plot the model architecture, displaying the shapes of each layer."
      ],
      "metadata": {
        "id": "5yG1V6Zrml5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model3.summary()"
      ],
      "metadata": {
        "id": "OV4MhFP1Dy5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With residual connections, you can build networks of arbitrary depth, without having to worry about vanishing gradients. We will see an example later.\n",
        "\n",
        "**Intuitions on why residual blocks work:**\n",
        "*   Shorter path for gradients\n",
        "*   Easy to learn the identity matrix\n",
        "*   Ensemble of shallow networks"
      ],
      "metadata": {
        "id": "7rycg6y3ehvl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Batch Normalization**\n",
        "*   Adaptively normalize data even as the mean and variance change over time during training\n",
        "*   During training, it uses the mean and variance of the current batch of data to normalize samples\n",
        "*    During inference (when a big enough batch of representative data may not be available), it uses an exponential moving average of the batch-wise mean and variance of the data seen during training.\n"
      ],
      "metadata": {
        "id": "v9dpxDKLtbkG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div>\n",
        "<img src=\"https://miro.medium.com/max/1153/1*xQhPvRh08oKFC63swgWr_w.png\" width=\"500\"/>\n",
        "</div>"
      ],
      "metadata": {
        "id": "gTucnUKguvcW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's try to calculate the no. of params introduced because of batch normalization in following example."
      ],
      "metadata": {
        "id": "mbssJsObuxz0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model with a batch normalization layer"
      ],
      "metadata": {
        "id": "5TClF7208pG7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model with a batch normalization layer\n",
        "\n",
        "inputs = keras.Input(shape=(32, 32, 3))  # Define the input layer with shape 32x32x3 for an RGB image.\n",
        "\n",
        "# Because the output of the Conv2D layer gets normalized, the layer doesn’t need its own bias vector\n",
        "x = layers.Conv2D(32, 3, activation=\"relu\", use_bias=False)(inputs)\n",
        "# Apply a 2D convolutional layer with 32 filters and a 3x3 kernel, using ReLU activation.\n",
        "# `use_bias=False` since Batch Normalization will handle normalization, removing the need for a bias term in this layer.\n",
        "\n",
        "x = layers.BatchNormalization()(x)\n",
        "# Apply batch normalization to standardize the output of the convolutional layer.\n",
        "# This layer normalizes the activations, improving training stability and model performance.\n",
        "\n",
        "x = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)\n",
        "# Add another 2D convolutional layer with 64 filters and a 3x3 kernel, using ReLU activation.\n",
        "# Padding is set to 'same' to maintain spatial dimensions.\n",
        "\n",
        "model4 = keras.Model(inputs=inputs, outputs=x)  # Define the model with specified input and final output layer.\n",
        "\n",
        "plot_model(model4, show_shapes=True)  # Plot the model architecture, displaying each layer's shape."
      ],
      "metadata": {
        "id": "h5jAYBOip2so"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q: How many parameters does the BN layer introduce in the above model?\n",
        "\n",
        "A: 128\n",
        "\n",
        "\n",
        "Q: HOW?\n",
        "\n",
        "A: Batch Normalization layer introduces **four parameters** per channel. However, only **two parameters**, γ and β, are **learnable/trainable parameters** used to apply scaling and shifting to the transformation. The remaining **two parameters**, moving_mean and moving_variance, are **non-trainable** and are directly calculated from the mean across the batch and saved as part of the state of the Batch Normalization layer.\n",
        "\n",
        "Here, the number of channels in the preceding layer is 32. Hence, the total number of parameters is equal to 32 * 4 = 128, but out of this, only 128/2 = 64 are trainable."
      ],
      "metadata": {
        "id": "Vu9iWI1DljEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model4.summary()"
      ],
      "metadata": {
        "id": "MtKeg1J9tVk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Intuitions:**\n",
        "*  Batch Normalization is also a (weak) regularization method.\n",
        "    - increases no. of params\n",
        "    - but also adds noise ~ data augmentation ~ dropout"
      ],
      "metadata": {
        "id": "4YLehpMpxJMN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Depthwise separable convolutions**\n",
        "\n",
        "* This layer performs a **spatial convolution [Depthwise Conv.]** on each channel of\n",
        "its input, independently, before mixing output channels via a **pointwise convolution**.\n",
        "\n",
        "* **Depthwise separable convolution = Depthwise Conv. +  Pointwise Conv.**\n",
        "\n",
        "* This makes your model smaller and  acts as a strong prior. We impose a strong prior by assuming that spatial patterns and cross-channel patterns can be modeled separately.This is equivalent to separating the learning of spatial features and the learning of channel-wise features.\n",
        "* Depthwise separable convolution relies on the assumption that spatial locations in intermediate activations\n",
        "are highly correlated, but different channels are highly independent. So we never use depthwise separable convolution after the input layer. Because RGB channels are **highly correlated**."
      ],
      "metadata": {
        "id": "4AqwncgPvQju"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<figure>\n",
        "<center>\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/5/56/RGB_channels_separation.png\" width=\"400\"/><img src=\"https://drive.google.com/uc?export=view&id=1e4h4NdbHRCxB1Oe_eoNhQQ6ZbAERf22K\" width=\"500\"/> <figcaption>## RGB channels are highly correlated. ##________________## Depthwise separable convolutions ##</figcaption>\n",
        "\n",
        "</center>\n",
        "</figure>\n"
      ],
      "metadata": {
        "id": "ezUdJ7lJ4f-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's quickly look at the code first"
      ],
      "metadata": {
        "id": "9__AMGKUS6bb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary layers from Keras\n",
        "inputs = keras.Input(shape=(32, 32, 3))  # Define the input shape of the model (32x32 image with 3 channels, e.g., RGB)\n",
        "x = layers.Conv2D(32, 3, activation=\"relu\")(inputs)  # Applying a regular Conv2D layer with 32 filters, a 3x3 kernel, and ReLU activation function\n",
        "\n",
        "# Separable convolution: Depthwise and pointwise convolutions\n",
        "x = layers.SeparableConv2D(64, 3, activation=\"relu\", padding=\"same\")(x)  # Applying a separable convolution with 64 filters, 3x3 kernel, ReLU activation, and 'same' padding\n",
        "\n",
        "# Defining the model by specifying input and output\n",
        "sep_model = keras.Model(inputs=inputs, outputs=x)\n",
        "\n",
        "# Plotting the model to visualize its architecture with layer shapes\n",
        "plot_model(sep_model, show_shapes=True)\n",
        "\n",
        "# Printing a summary of the model, which includes the number of parameters for each layer\n",
        "sep_model.summary()\n",
        "\n",
        "# Q: Verify the number of parameters in the separable_conv2D layer.\n",
        "# A: Explanation of the parameter calculation for the SeparableConv2D layer:\n",
        "#    - Depthwise convolution: (32 filters * 3x3 kernel) = 32 * (3*3) = 288 parameters (for each channel).\n",
        "#    - Pointwise convolution: (32 input channels * 1x1 kernel * 64 output filters) = 32 * 1 * 1 * 64 = 2048 parameters.\n",
        "#    - Biases: Each output filter has a bias term, so the number of biases is 64.\n",
        "#    Total parameters: 288 (depthwise) + 2048 (pointwise) + 64 (biases) = 2400 parameters.\n",
        "# Note: In Keras' SeparableConv2D, the bias in the depthwise convolution is not included, which is why it's excluded in the first calculation.\n",
        "\n",
        "# Considering all biases:\n",
        "#    Total parameters when including the bias term in the depthwise convolution:\n",
        "#    - Depthwise bias: 32 (for each input channel).\n",
        "#    - Total parameters: 288 (depthwise weights) + 32 (depthwise biases) + 2048 (pointwise weights) + 64 (pointwise biases) = 2432."
      ],
      "metadata": {
        "id": "vwVyRlQ8uOLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compare the above with a model where we replace the SeparableConv2D with a Conv2D layer."
      ],
      "metadata": {
        "id": "XCsZV3LCxYJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the input shape of the model (32x32 image with 3 channels, e.g., RGB)\n",
        "inputs = keras.Input(shape=(32, 32, 3))\n",
        "\n",
        "# Applying a regular Conv2D layer with 32 filters, 3x3 kernel, and ReLU activation function\n",
        "x = layers.Conv2D(32, 3, activation=\"relu\")(inputs)\n",
        "\n",
        "# Applying another Conv2D layer with 64 filters, 3x3 kernel, ReLU activation, and 'same' padding\n",
        "x = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)\n",
        "\n",
        "# Defining the model by specifying input and output\n",
        "model = keras.Model(inputs=inputs, outputs=x)\n",
        "\n",
        "# Plotting the model to visualize its architecture with layer shapes\n",
        "plot_model(model, show_shapes=True)\n",
        "\n",
        "# Printing a summary of the model, which includes the number of parameters for each layer\n",
        "model.summary()\n",
        "\n",
        "# Q: Why does sep_model have much fewer parameters than this model?\n",
        "# A: The SeparableConv2D layer in sep_model performs depthwise and pointwise convolutions independently:\n",
        "#    - Depthwise convolution applies a filter for each input channel individually (no mixing across channels).\n",
        "#    - Pointwise convolution applies a 1x1 filter to combine information across all channels.\n",
        "# In contrast, the Conv2D layers in this model apply 2D convolutions across all input channels at once, resulting in more parameters.\n",
        "# This makes the SeparableConv2D layer more parameter-efficient compared to standard Conv2D."
      ],
      "metadata": {
        "id": "0bS1qIoywrAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **A mini Xception-like model**"
      ],
      "metadata": {
        "id": "rfYMjxYWyOL5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll build a model like the Xception model, but a smaller version.\n",
        "\n",
        "But first let's see what the actual Xception model looks like."
      ],
      "metadata": {
        "id": "tQkHVp72IRkX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![picture](https://miro.medium.com/max/833/1*t6qfo9ucYza_lbLfg5-p_w.png)"
      ],
      "metadata": {
        "id": "OEYS-LaWIDql"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q: In middle- flow blocks, what arguments do you give to the SepConv layer ?\n",
        "\n",
        "A: HW question\n"
      ],
      "metadata": {
        "id": "yXmbHNJAWhMd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's use the cats-vs-dogs data and create datasets."
      ],
      "metadata": {
        "id": "FQYxgWbwIvLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the root directory path where the dataset is stored\n",
        "data_dir = '/content/cats_vs_dogs_small'  # This is the main directory containing the dataset\n",
        "\n",
        "# Defining the path for the training data\n",
        "train_path = data_dir + '/train'  # Path for the training dataset, assuming the directory has subfolders for images\n",
        "\n",
        "# Defining the path for the validation data\n",
        "validation_path = data_dir + '/validation'  # Path for the validation dataset\n",
        "\n",
        "# Defining the path for the test data\n",
        "test_path = data_dir + '/test'  # Path for the test dataset"
      ],
      "metadata": {
        "id": "TUfhxMyOkrA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the utility function to create datasets from a directory\n",
        "from tensorflow.keras.utils import image_dataset_from_directory\n",
        "\n",
        "# Loading the training dataset from the 'train' directory\n",
        "train_dataset = image_dataset_from_directory(\n",
        "               train_path,  # Path to the training data\n",
        "               image_size=(180, 180),  # Resizing images to 180x180 pixels\n",
        "               batch_size=32)  # Setting the batch size to 32 for training\n",
        "\n",
        "# Loading the validation dataset from the 'validation' directory\n",
        "validation_dataset = image_dataset_from_directory(\n",
        "                      validation_path,  # Path to the validation data\n",
        "                      image_size=(180, 180),  # Resizing images to 180x180 pixels\n",
        "                      batch_size=32)  # Setting the batch size to 32 for validation\n",
        "\n",
        "# Loading the test dataset from the 'test' directory\n",
        "test_dataset = image_dataset_from_directory(\n",
        "                test_path,  # Path to the test data\n",
        "                image_size=(180, 180),  # Resizing images to 180x180 pixels\n",
        "                batch_size=32)  # Setting the batch size to 32 for testing"
      ],
      "metadata": {
        "id": "CVOWgUhECt5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries from Keras\n",
        "import keras\n",
        "from keras import layers\n",
        "\n",
        "# Defining the input layer with shape 180x180 pixels and 3 color channels (RGB)\n",
        "inputs = keras.Input(shape=(180, 180, 3))\n",
        "\n",
        "# Normalizing the pixel values to the range [0, 1] by dividing by 255\n",
        "x = layers.Rescaling(1./255)(inputs)\n",
        "\n",
        "# Applying a regular Conv2D layer with 32 filters, a 5x5 kernel, and no bias (filter weights only)\n",
        "x = layers.Conv2D(filters=32, kernel_size=5, use_bias=False)(x)\n",
        "\n",
        "# Q: Why not use depth-wise separable convolution here?\n",
        "# A: RGB channels in input images are highly correlated, so regular convolution is preferred for capturing complex patterns between channels in the initial layers.\n",
        "\n",
        "# Repeated block structure, common in deep learning models to gradually increase feature map complexity\n",
        "for size in [32, 64, 128, 256, 512]:  # Loop through different filter sizes (32, 64, 128, 256, 512)\n",
        "    residual = x  # Storing the current output for skip connection (residual block)\n",
        "\n",
        "    # Applying BatchNormalization to stabilize learning by normalizing activations\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    # ReLU activation for non-linearity\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "    # Applying SeparableConv2D layer for more efficient convolutions with reduced parameters\n",
        "    x = layers.SeparableConv2D(size, 3, padding=\"same\", use_bias=False)(x)\n",
        "\n",
        "    # Applying BatchNormalization and ReLU activation again\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "    # Another SeparableConv2D layer\n",
        "    x = layers.SeparableConv2D(size, 3, padding=\"same\", use_bias=False)(x)\n",
        "\n",
        "    # MaxPooling operation to reduce the spatial dimensions (down-sampling)\n",
        "    x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
        "\n",
        "    # Skip connection (residual) for the current block\n",
        "    residual = layers.Conv2D(size, 1, strides=2, padding=\"same\", use_bias=False)(residual)\n",
        "\n",
        "    # Adding the skip connection to the current output\n",
        "    x = layers.add([x, residual])\n",
        "\n",
        "# GlobalAveragePooling2D reduces the spatial dimensions to a single value per feature map\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "\n",
        "# Dropout for regularization, reducing overfitting by randomly setting a fraction of input units to 0 during training\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "# Output layer with a sigmoid activation function, suitable for binary classification\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "# Defining the model with the specified inputs and outputs\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "UnZGLjDhycgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model(model)"
      ],
      "metadata": {
        "id": "BIvwflhlc9Q-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDIn4pcAyjcC"
      },
      "outputs": [],
      "source": [
        "# Compiling the model with the specified loss function, optimizer, and evaluation metric\n",
        "model.compile(\n",
        "    loss=\"binary_crossentropy\",  # Binary Crossentropy loss function, suitable for binary classification tasks\n",
        "    optimizer=\"rmsprop\",         # RMSprop optimizer, an adaptive learning rate method that works well for CNNs\n",
        "    metrics=[\"accuracy\"]         # Accuracy as the evaluation metric to track performance during training\n",
        ")\n",
        "\n",
        "# Training the model using the training dataset\n",
        "history = model.fit(\n",
        "    train_dataset,               # The training dataset to train the model\n",
        "    epochs=100,                  # Number of epochs (iterations over the entire dataset)\n",
        "    validation_data=validation_dataset  # The validation dataset to evaluate the model's performance after each epoch\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIhBNi09EvQ0"
      },
      "outputs": [],
      "source": [
        "# Converting the training history to a Pandas DataFrame for easier manipulation\n",
        "data = pd.DataFrame(history.history)\n",
        "\n",
        "# Plotting the training accuracy ('accuracy') over epochs\n",
        "plt.plot(range(1, len(data) + 1), data['accuracy'], 'bo', label=\"Training accuracy\")  # 'bo' = blue circles for training accuracy\n",
        "plt.plot(range(1, len(data) + 1), data['val_accuracy'], 'b', label=\"Validation accuracy\")  # 'b' = blue line for validation accuracy\n",
        "plt.legend()  # Display the legend to label the curves\n",
        "plt.xlabel(\"Epochs\")  # Label for the x-axis (epochs)\n",
        "plt.ylabel(\"Accuracy\")  # Label for the y-axis (accuracy)\n",
        "plt.show()  # Display the accuracy plot\n",
        "\n",
        "# Creating a new figure for the loss plot\n",
        "plt.figure()\n",
        "\n",
        "# Plotting the training loss ('loss') over epochs\n",
        "plt.plot(range(1, len(data) + 1), data['loss'], 'bo', label=\"Training loss\")  # 'bo' = blue circles for training loss\n",
        "plt.plot(range(1, len(data) + 1), data['val_loss'], 'b', label=\"Validation loss\")  # 'b' = blue line for validation loss\n",
        "plt.legend()  # Display the legend to label the curves\n",
        "plt.xlabel(\"Epochs\")  # Label for the x-axis (epochs)\n",
        "plt.ylabel(\"Loss\")  # Label for the y-axis (loss)\n",
        "plt.show()  # Display the loss plot"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Interpreting what ConvNets learn\n",
        " * Visualizing intermediate ConvNetsoutputs (intermediate activations)\n",
        " * Visualizing ConvNets filters\n",
        " * Visualizing heatmaps of class activation in an image\n",
        "\n",
        "NOTE:\n",
        "* We will focus mostly on the concepts and key ideas\n",
        "* A lot of the code is pre-processing and post-processing. We will not spend time on these parts.\n",
        "* We will see the part of the code that implements the key ideas.\n",
        "\n",
        "Pro:\n",
        "*   Developing ideas\n",
        "*   Developing thought process"
      ],
      "metadata": {
        "id": "UHFkjDr4eAZV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Visualizing intermediate activations**\n",
        "\n",
        "The output of a layer is called its 'activation'(It's the output of the activation function).\n",
        "\n",
        "These activations can be visualized by plotting the feature maps.\n",
        "\n",
        "![picture](https://drive.google.com/uc?export=view&id=1vT8e59AYTFRlrrI3C-iUHTctxyhfBiJJ)\n",
        "\n",
        "We will plot each feature map independently as a 2D image, since they encode relatively indepent features."
      ],
      "metadata": {
        "id": "TtyRbwd7eKAE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the Keras API from TensorFlow for model building and manipulation\n",
        "from tensorflow import keras\n",
        "\n",
        "# Importing the plot_model utility from Keras to visualize the model architecture\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "# Importing numpy for numerical operations, often used with arrays and matrices in machine learning\n",
        "import numpy as npD:\\Dominic\\AI_MLOps\\Colab\n",
        "\n",
        "# Importing matplotlib for plotting graphs and visualizations\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Importing the layers module from Keras to build the model by adding layers such as Conv2D, Dense, etc.\n",
        "from tensorflow.keras import layers  # <----- Note this"
      ],
      "metadata": {
        "id": "W6EpNnTPtWcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model to be used for visualization :** We are going to use a trained model from the previous assignment -  M2_AST_01_Convolutional _Neural_Networks. In that assignment, we created one model with augmentation and saved it through a callback function with the name \"convnet_from_scratch_with_augmentation_keras\". You can download that model from there and use it by providing the proper path after loading it.\n",
        "\n",
        "For the sake of simplicity, we have already provided that model and has been downloaded along with the dataset."
      ],
      "metadata": {
        "id": "mrmyY_4uF8rE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading a pre-trained model from a file (assuming the model is saved as 'convnet_from_scratch_with_augmentation_keras.keras')\n",
        "model = keras.models.load_model('convnet_from_scratch_with_augmentation_keras.keras')\n",
        "\n",
        "# Visualizing the architecture of the loaded model using plot_model\n",
        "plot_model(model)  # This will display a graphical representation of the model's layers and structure"
      ],
      "metadata": {
        "id": "zoNajzD7pntO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the model's performance on the test dataset\n",
        "test_loss, test_acc = model.evaluate(test_dataset)  # Evaluates the model on test data and returns the loss and accuracy\n",
        "\n",
        "# Printing the test accuracy to display the result\n",
        "print(f\"Test accuracy is:{test_acc:.3f}\")  # Prints the test accuracy rounded to 3 decimal places"
      ],
      "metadata": {
        "id": "2dky0dGDqoew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Getting the image &  Preprocessing** that will be passsed inside the model for visualization ."
      ],
      "metadata": {
        "id": "LSL_9iBEGlZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading the image of a cat from the specified URL and saving it as \"cat.jpg\"\n",
        "img_path = keras.utils.get_file(fname=\"cat.jpg\", origin=\"https://img-datasets.s3.amazonaws.com/cat.jpg\")\n",
        "\n",
        "# Function to preprocess the image into an array suitable for input into a model\n",
        "def get_img_array(img_path, target_size):\n",
        "    # Loading the image from the path and resizing it to the target size (180x180)\n",
        "    img = keras.utils.load_img(img_path, target_size=target_size)\n",
        "\n",
        "    # Converting the loaded image into a numpy array\n",
        "    array = keras.utils.img_to_array(img)  # Converts image to a 3D numpy array (height, width, channels)\n",
        "\n",
        "    # Adding an extra dimension to create a batch of one sample\n",
        "    # This changes the shape from (height, width, channels) to (1, height, width, channels)\n",
        "    array = np.expand_dims(array, axis=0)  # The shape is now (1, 180, 180, 3)\n",
        "\n",
        "    # Returning the processed image array\n",
        "    return array\n",
        "\n",
        "# Calling the function to preprocess the downloaded image\n",
        "img_tensor = get_img_array(img_path, target_size=(180, 180))  # Resize the image to (180, 180)\n",
        "\n",
        "# Displaying the image\n",
        "plt.axis(\"off\")  # Disable axis to show the image clearly\n",
        "plt.imshow(img_tensor[0].astype(\"uint8\"))  # Convert the image tensor back to uint8 for display\n",
        "plt.show()  # Display the image"
      ],
      "metadata": {
        "id": "vvkpXx6KhZng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Focus here: Instantiate a model that returns the activations of specific layers\n",
        "\n",
        "# Initialize lists to store the outputs and names of layers that we are interested in\n",
        "layer_outputs = []\n",
        "layer_names = []\n",
        "\n",
        "# Loop through each layer in the original model to identify Conv2D or MaxPooling2D layers\n",
        "for layer in model.layers:\n",
        "    # Check if the layer is a Conv2D or MaxPooling2D layer\n",
        "    if isinstance(layer, (layers.Conv2D, layers.MaxPooling2D)):\n",
        "        layer_outputs.append(layer.output)  # Add the output of the layer to the layer_outputs list\n",
        "        layer_names.append(layer.name)  # Add the layer's name to the layer_names list\n",
        "\n",
        "# Creating a new model that outputs the activations from the selected layers\n",
        "activation_model = keras.Model(inputs=model.input, outputs=layer_outputs)  # Output will be the activations of Conv2D and MaxPooling2D layers\n",
        "\n",
        "# Visualize the architecture of the new model that returns activations of the selected layers\n",
        "plot_model(activation_model)"
      ],
      "metadata": {
        "id": "A18axaoihdg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "activation_model.summary()"
      ],
      "metadata": {
        "id": "wtSTumbvHGfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the activations of the selected layers (Conv2D and MaxPooling2D) for the input image\n",
        "activations = activation_model.predict(img_tensor)  # Predict the activations for the input image tensor (batch of 1 image)\n",
        "\n",
        "# Print the number of outputs (activations) returned by the model\n",
        "print(f\"No. of outputs= {len(activations)}\")\n",
        "\n",
        "# Get the activations of the first layer (Conv2D or MaxPooling2D layer) in the model\n",
        "first_layer_feature_maps = activations[0]  # activations[0] corresponds to the first layer's activations in the list\n",
        "\n",
        "# Print the shape of the first layer's activations (feature maps)\n",
        "print(f\"first_layer_activation.shape= {first_layer_feature_maps.shape}\")"
      ],
      "metadata": {
        "id": "7z909nZiiTxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the activation of the first feature map from the first layer\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Visualize the first feature map of the first layer's activations using 'viridis' colormap\n",
        "plt.matshow(first_layer_feature_maps[0, :, :, 0], cmap=\"viridis\")  # Accessing the 1st feature map (index 0 in the last dimension)\n",
        "\n",
        "# Display the image\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lML8McG4k0L9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems that the filter has detected ______.\n",
        "\n",
        "Let's look at a feature map after each layer."
      ],
      "metadata": {
        "id": "zqjzaGrX0vZg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the activations of the 3rd feature map (index 2) for the first 9 layers\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Loop through the first 9 layers' activations and visualize the 3rd feature map (index 2) for each\n",
        "for i in range(9):\n",
        "    # Access the 3rd feature map (index 2) of the i-th layer's activations and visualize it\n",
        "    plt.matshow(activations[i][0, :, :, 2], cmap=\"viridis\")  # '0' selects the first (and only) image in the batch, '2' selects the 3rd feature map\n",
        "    #plt.show()\n"
      ],
      "metadata": {
        "id": "iHK7sEtx2IUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note the dimensions on the above images. Successive feature maps are actually of smaller dimensions but scaled to be the same size during visualization."
      ],
      "metadata": {
        "id": "wXJOiFWx1Bg0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's visualise all the feature maps of all the layers."
      ],
      "metadata": {
        "id": "5SC0nlq_3Lb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Post-processing code - visualization of every channel in every intermediate activation\n",
        "\n",
        "images_per_row = 16  # Set the number of images to display per row in the grid\n",
        "\n",
        "# Iterate over each layer's name and activation output\n",
        "for layer_name, layer_activation in zip(layer_names, activations):\n",
        "    n_features = layer_activation.shape[-1]  # Number of feature maps (channels) in the current layer's activation\n",
        "    size = layer_activation.shape[1]  # Height and width of the feature maps\n",
        "    n_cols = n_features // images_per_row  # Number of columns in the grid based on the number of feature maps\n",
        "\n",
        "    # Create a blank display grid to place the feature maps\n",
        "    display_grid = np.zeros(((size + 1) * n_cols - 1, images_per_row * (size + 1) - 1))\n",
        "\n",
        "    # Loop through each feature map (channel) and place it in the grid\n",
        "    for col in range(n_cols):\n",
        "        for row in range(images_per_row):\n",
        "            channel_index = col * images_per_row + row  # Calculate the index of the current feature map\n",
        "            channel_image = layer_activation[0, :, :, channel_index].copy()  # Get the feature map for the current channel\n",
        "\n",
        "            # Normalize and adjust the feature map for better visualization\n",
        "            if channel_image.sum() != 0:  # Avoid division by zero\n",
        "                channel_image -= channel_image.mean()  # Subtract the mean to center the data\n",
        "                channel_image /= channel_image.std()  # Normalize by the standard deviation\n",
        "                channel_image *= 64  # Scale the values for better contrast\n",
        "                channel_image += 128  # Shift values to a visible range\n",
        "            channel_image = np.clip(channel_image, 0, 255).astype(\"uint8\")  # Clip values to valid pixel range (0-255)\n",
        "\n",
        "            # Place the processed feature map into the display grid\n",
        "            display_grid[\n",
        "                col * (size + 1): (col + 1) * size + col,\n",
        "                row * (size + 1): (row + 1) * size + row] = channel_image\n",
        "\n",
        "    # Scale the grid and plot it\n",
        "    scale = 1. / size  # Calculate scale factor based on feature map size\n",
        "    plt.figure(figsize=(scale * display_grid.shape[1], scale * display_grid.shape[0]))  # Set figure size based on grid dimensions\n",
        "    plt.title(layer_name)  # Set title to the layer name\n",
        "    plt.grid(False)  # Remove grid lines for clarity\n",
        "    plt.axis(\"off\")  # Remove axis for better visualization\n",
        "    plt.imshow(display_grid, aspect=\"auto\", cmap=\"viridis\")  # Display the feature maps with the 'viridis' colormap"
      ],
      "metadata": {
        "id": "sov3-WP2k3aT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The first layer acts as a collection of various edge detectors.\n",
        "\n",
        "* As you go deeper, the activations become increasingly abstract and less visually interpretable. They begin to encode higher-level concepts such as “cat ear” and “cat eye.”\n",
        "\n",
        "* The sparsity of the activations increases with the depth of the layer: in the first layer, almost all filters are activated by the input image, but in the following layers, more and more filters are blank. This means the pattern encoded by the filter isn’t found in the input image"
      ],
      "metadata": {
        "id": "vDL2xEBypLwG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Visualising ConvNet filters**\n",
        "*   Pick a filter\n",
        "*   Ask the question: What kind of an input image will excite the filter?\n",
        "*   What should the input image be so that you see a (yellow) feature map?\n",
        "*   In other words, we want to visualize those patterns in the input image that the filter picks up and results in high (yellow) values in the feature map.\n"
      ],
      "metadata": {
        "id": "EVsagdgdquAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiating the Xception convolutional base\n",
        "\n",
        "model = keras.applications.xception.Xception( weights=\"imagenet\", include_top=False)"
      ],
      "metadata": {
        "id": "VA0-i2t6k69H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Key Points**:\n",
        "\n",
        "1. `keras.applications.xception.Xception`:\n",
        "\n",
        "* This is a pre-defined model in Keras based on the Xception architecture, which is a deep convolutional neural network (CNN) model. It is designed for image classification tasks.\n",
        "\n",
        "* Xception stands for \"Extreme Inception\" and is a more advanced version of the Inception architecture, using depthwise separable convolutions.\n",
        "\n",
        "2. `weights=\"imagenet\"`:\n",
        "\n",
        "* This argument loads the pre-trained weights from the ImageNet dataset. ImageNet is a large dataset of labeled images used for training deep learning models for image classification tasks. The pre-trained weights help the model generalize well to new data without having to train it from scratch.\n",
        "\n",
        "3. `include_top=False`:\n",
        "\n",
        "* This argument tells Keras not to include the top fully connected layers (also called the \"classification head\") of the Xception model.\n",
        "* By setting `include_top=False`, you obtain only the convolutional base of the model, which is typically used for feature extraction. The model can then be fine-tuned or adapted for a new task (e.g., adding custom layers for classification)."
      ],
      "metadata": {
        "id": "lYWAM9Gcmlmq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q: Printing the names of  conv and sepConv layers in Xception\n",
        "\n",
        "for layer in model.layers:\n",
        "    if isinstance(layer, (keras.layers.Conv2D, keras.layers.SeparableConv2D)):\n",
        "        print(layer.name)"
      ],
      "metadata": {
        "id": "1fkAZk_fyIaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a feature extractor model\n",
        "\n",
        "# Define the name of the layer to extract features from\n",
        "layer_name = \"block3_sepconv1\"  # This is the name of the specific layer in the Xception model.\n",
        "\n",
        "# Get the actual layer object by its name from the model\n",
        "layer = model.get_layer(name=layer_name)  # Retrieves the layer object by its name.\n",
        "\n",
        "# Create a new model that takes the same input as the original model, but outputs the specified layer's output\n",
        "feature_extractor = keras.Model(inputs=model.input, outputs=layer.output)\n",
        "# The feature_extractor model will output the activations (features) from the 'block3_sepconv1' layer.\n",
        "\n",
        "# Display the summary of the feature extractor model\n",
        "feature_extractor.summary()  # Prints the summary of the new model, showing its layers and the shape of the output.\n"
      ],
      "metadata": {
        "id": "rb_zmZoNyIc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q: What is the last layer ?\n",
        "\n",
        "Q: How many filters does block3_sepconv1 have?\n",
        "\n",
        "Q: Why are there so many Nones in the shapes?"
      ],
      "metadata": {
        "id": "q33H-EUi7aeB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the feature extractor\n",
        "\n",
        "activation = feature_extractor(keras.applications.xception.preprocess_input(img_tensor))\n",
        "# Image is preprocessed specific to Inception model before passing inside the feature_extractor"
      ],
      "metadata": {
        "id": "lB30XfKvyIff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here comes the key idea:\n",
        "*   Define an objective function: mean pixel value of feature map\n",
        "*   Use gradient \"Ascent\" on the \"input image space\" to maximize this objective\n",
        "Here's an analogy: (drawing)"
      ],
      "metadata": {
        "id": "aTnndxqY6sTc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Define a function to compute the loss for a specific filter's activation\n",
        "def compute_loss(image, filter_index):  # Q: How many indices do we have? A: 256\n",
        "    # Get the activations from the feature_extractor model for the input image\n",
        "    activation = feature_extractor(image)\n",
        "\n",
        "    # Extract the activation corresponding to the selected filter\n",
        "    # We slice out the boundaries (2 pixels from each side) to avoid edge effects\n",
        "    filter_activation = activation[:, 2:-2, 2:-2, filter_index]  # Leaving out the boundaries\n",
        "\n",
        "    # Return the mean activation value for the selected filter\n",
        "    return tf.reduce_mean(filter_activation)"
      ],
      "metadata": {
        "id": "fLxwawZfyIh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss maximization via stochastic gradient ascent\n",
        "\n",
        "@tf.function  # This decorator compiles the function into a graph for optimized execution\n",
        "def gradient_ascent_step(image, filter_index, learning_rate):\n",
        "    # Use TensorFlow's GradientTape to record the operations for gradient computation\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(image)  # Watch the image tensor to track its gradients\n",
        "        loss = compute_loss(image, filter_index)  # Compute the loss based on the filter activation\n",
        "\n",
        "    # Compute the gradient of the loss with respect to the image\n",
        "    grads = tape.gradient(loss, image)    # Q: Is the gradient a vector or scalar? A: vector\n",
        "    grads = tf.math.l2_normalize(grads)    # Normalize the gradient to avoid exploding gradients\n",
        "\n",
        "    # Update the image in the direction of the gradient (gradient ascent)\n",
        "    image += learning_rate * grads          # Q: What makes this gradient \"ascent\"? A: plus sign\n",
        "\n",
        "    return image"
      ],
      "metadata": {
        "id": "-_amLc3KyIj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate filter visualizations\n",
        "\n",
        "img_width = 200  # Width of the generated image for filter visualization\n",
        "img_height = 200  # Height of the generated image for filter visualization\n",
        "\n",
        "def generate_filter_pattern(filter_index):\n",
        "    iterations = 30  # Number of iterations to perform gradient ascent\n",
        "    learning_rate = 10.  # Learning rate for the gradient ascent update\n",
        "    # Initialize a random image with pixel values between 0.4 and 0.6\n",
        "    image = tf.random.uniform(\n",
        "        minval=0.4,  # Lower bound for random initialization\n",
        "        maxval=0.6,  # Upper bound for random initialization\n",
        "        shape=(1, img_width, img_height, 3)  # Shape of the image (1 image, 200x200 pixels, 3 channels)\n",
        "    )\n",
        "\n",
        "    # Perform gradient ascent for 'iterations' steps\n",
        "    for i in range(iterations):\n",
        "        image = gradient_ascent_step(image, filter_index, learning_rate)  # Update the image to maximize filter activation\n",
        "\n",
        "    return image[0].numpy()  # Return the final image as a NumPy array (drop the batch dimension)"
      ],
      "metadata": {
        "id": "nHshMEN4yIml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility function to convert a tensor into a valid image\n",
        "\n",
        "def deprocess_image(image):\n",
        "    # Subtract the mean of the image to center the pixel values around zero\n",
        "    image -= image.mean()\n",
        "\n",
        "    # Normalize the pixel values to have a standard deviation of 1\n",
        "    image /= image.std()\n",
        "\n",
        "    # Multiply by 64 to scale the pixel values\n",
        "    image *= 64\n",
        "\n",
        "    # Add 128 to shift the pixel values back to a standard range\n",
        "    image += 128\n",
        "\n",
        "    # Clip the pixel values to stay within the valid range [0, 255] for display\n",
        "    image = np.clip(image, 0, 255).astype(\"uint8\")\n",
        "\n",
        "    # Crop the borders to remove unwanted pixels (usually artifacts from the gradient ascent)\n",
        "    image = image[25:-25, 25:-25, :]\n",
        "\n",
        "    return image\n",
        "\n",
        "# Visualizing the filter pattern generated for filter index 2\n",
        "plt.axis(\"off\")  # Turn off axis to focus on the image itself\n",
        "plt.imshow(deprocess_image(generate_filter_pattern(filter_index=2)))  # Generate and display the processed image\n"
      ],
      "metadata": {
        "id": "oSnWkDvN0EZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Post-processing - Just visualization\n",
        "# Generating a grid of all filter response patterns in a layer\n",
        "\n",
        "# List to hold all processed filter images\n",
        "all_images = []\n",
        "for filter_index in range(64):  # Looping through the filters in the layer\n",
        "    print(f\"Processing filter {filter_index}\")\n",
        "\n",
        "    # Generate and deprocess the filter pattern for the current filter index\n",
        "    image = deprocess_image(\n",
        "        generate_filter_pattern(filter_index)\n",
        "    )\n",
        "\n",
        "    # Append the processed image to the list\n",
        "    all_images.append(image)\n",
        "\n",
        "# Defining parameters for the grid layout\n",
        "margin = 5  # Space between the filter images\n",
        "n = 8  # Number of filters in each row and column (8x8 grid)\n",
        "cropped_width = img_width - 25 * 2  # Cropping width of the image to remove boundaries\n",
        "cropped_height = img_height - 25 * 2  # Cropping height of the image to remove boundaries\n",
        "width = n * cropped_width + (n - 1) * margin  # Total width of the grid\n",
        "height = n * cropped_height + (n - 1) * margin  # Total height of the grid\n",
        "\n",
        "# Initialize an empty image array to stitch the filter patterns together\n",
        "stitched_filters = np.zeros((width, height, 3))\n",
        "\n",
        "# Stitches the filter images into a grid layout\n",
        "for i in range(n):  # Loop over rows\n",
        "    for j in range(n):  # Loop over columns\n",
        "        image = all_images[i * n + j]  # Select the filter image\n",
        "        # Place the image at the appropriate location in the grid\n",
        "        stitched_filters[\n",
        "            (cropped_width + margin) * i : (cropped_width + margin) * i + cropped_width,\n",
        "            (cropped_height + margin) * j : (cropped_height + margin) * j + cropped_height,\n",
        "            :\n",
        "        ] = image\n",
        "\n",
        "# Save the stitched filter grid as an image\n",
        "keras.utils.save_img(\n",
        "    f\"filters_for_layer_{layer_name}.png\", stitched_filters)\n"
      ],
      "metadata": {
        "id": "rASYbJnX0HtR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.figure(figsize=(40,40))\n",
        "# plt.matshow(stitched_filters)\n",
        "\n",
        "for i in [0,8,16,32]:\n",
        "  plt.figure()\n",
        "  plt.imshow((all_images[i]))"
      ],
      "metadata": {
        "id": "mqQbBmvZ0Hwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![picture](https://drive.google.com/uc?export=view&id=1bwf3RIEp9yNTICbf1f5FWg9bX1H5BGNm)"
      ],
      "metadata": {
        "id": "rn1Ra77e4VIK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![picture](https://drive.google.com/uc?export=view&id=1VMtNw4qCs4BoN7d9Us8tNEtiKK_J4Csd)"
      ],
      "metadata": {
        "id": "EREwT4VT8vRN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![picture](https://drive.google.com/uc?export=view&id=1eXCejZ3bZP1rBwtLzUQ9RMO9N0tuqDCP)\n",
        "\n"
      ],
      "metadata": {
        "id": "OQLo9U5-8wBs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  **Visualizing heatmaps of class activation**\n",
        "\n",
        "* Visualise which parts of a given image led a ConvNet to its final classification decision\n",
        "* Such techniques are called **class activation map** (CAM) visualisation\n",
        "* Produce heatmaps of class activation over input images."
      ],
      "metadata": {
        "id": "cA6vN4Yb4ijq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example:\n",
        "\n",
        "![picture](https://drive.google.com/uc?export=view&id=1z1XY2GoYq_tEXhTXNMNT9qT3Yp2ic9ZR)"
      ],
      "metadata": {
        "id": "edfJ7xmG-p-v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First let's understand the idea behind CAM:\n",
        "\n",
        "(Remember: The CNN is already trained. Now we are just visualising aspects of the trained CNN)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "![picture](https://drive.google.com/uc?export=view&id=1-Kn8BsHj1rPr61NGdRy822o1fvQuxW3l)"
      ],
      "metadata": {
        "id": "lBl--ze7_go6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Note:\n",
        "*   One set of optimal weights for one class\n",
        "\n"
      ],
      "metadata": {
        "id": "VU9idU_-HBwr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's look at an improved version of CAM:\n",
        "\n",
        "\n",
        "*   Grad-CAM\n",
        "\n"
      ],
      "metadata": {
        "id": "8uRA385UHK82"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![picture](https://drive.google.com/uc?export=view&id=10QgdjWzPejhmCXLvzoXpf3_ySYx2BVCj)"
      ],
      "metadata": {
        "id": "BlKVYyd-__qi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key Take-away:\n",
        "\n",
        "\n",
        "*   Need the weights. Q: What are these weights?\n",
        "    - make up a weighted sum of featurmaps to get a heat map\n",
        "*   Can learn them through a new sub-problem- CAM\n",
        "*   Can compute them directly through gradients- grad-CAM\n",
        "\n"
      ],
      "metadata": {
        "id": "6wpNI2J4I9jw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the Xception network with pretrained weights\n",
        "\n",
        "model = keras.applications.xception.Xception(weights=\"imagenet\")"
      ],
      "metadata": {
        "id": "HOq9rVmh4rLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.summary()"
      ],
      "metadata": {
        "id": "CinrH57K5tdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing an input image for Xception\n",
        "\n",
        "# Download the image from the specified URL and save it locally to a temporary file\n",
        "img_path = keras.utils.get_file(fname=\"cat.jpg\",  # The filename to save the image as\n",
        "                                origin=\"https://img-datasets.s3.amazonaws.com/cat.jpg\")  # URL of the image to download\n",
        "\n",
        "# Function to load, resize, and preprocess the image for Xception input\n",
        "def get_img_array(img_path, target_size):\n",
        "    # Load the image from the given path and resize it to the target size (e.g., 299x299 for Xception)\n",
        "    img = keras.utils.load_img(img_path, target_size=target_size)\n",
        "\n",
        "    # Convert the loaded image into a NumPy array with shape (height, width, channels)\n",
        "    array = keras.utils.img_to_array(img)\n",
        "\n",
        "    # Add an extra dimension at the start of the array to represent the batch (shape becomes: (1, height, width, channels))\n",
        "    array = np.expand_dims(array, axis=0)\n",
        "\n",
        "    # Preprocess the image to match the Xception model's input requirements (normalization, centering, etc.)\n",
        "    array = keras.applications.xception.preprocess_input(array)  # Preprocess the image for Xception model\n",
        "\n",
        "    # Return the preprocessed image array ready for model input\n",
        "    return array\n",
        "\n",
        "# Call the function with the image path and the target size of (299, 299) as required by Xception\n",
        "img_array = get_img_array(img_path, target_size=(299, 299))  # Resize the image to 299x299 and preprocess it"
      ],
      "metadata": {
        "id": "QlFBo2dE4tQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicting the top three labels\n",
        "\n",
        "# Use the model to make a prediction on the preprocessed image (img_array)\n",
        "preds = model.predict(img_array)  # The model returns a prediction, typically the class probabilities\n",
        "\n",
        "# Decode the predictions to map the class indices to human-readable labels\n",
        "# 'decode_predictions' converts the model's output (predicted class probabilities) to actual class labels\n",
        "# It also returns the top predictions along with their probabilities\n",
        "print(keras.applications.xception.decode_predictions(preds, top=3)[0])  # Print the top 3 predicted labels and their probabilities"
      ],
      "metadata": {
        "id": "wVUyOcLu4vT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# printing out the top label\n",
        "np.argmax(preds[0])"
      ],
      "metadata": {
        "id": "oXruuv1D4xLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up a model that returns the last convolutional output\n",
        "\n",
        "# The layer name for the last convolutional layer in the Xception model\n",
        "last_conv_layer_name = \"block14_sepconv2_act\"\n",
        "\n",
        "# Specifying the layers that follow the convolutional layers (for classification)\n",
        "classifier_layer_names = [\"avg_pool\", \"predictions\"]\n",
        "\n",
        "# Retrieving the last convolutional layer from the model by its name\n",
        "last_conv_layer = model.get_layer(last_conv_layer_name)\n",
        "\n",
        "# Creating a new model that takes the original model's input and outputs the last convolutional layer's output\n",
        "last_conv_layer_model = keras.Model(model.inputs, last_conv_layer.output)\n",
        "\n",
        "# (Optional) Plot the model architecture up to the last convolutional layer to visualize the structure\n",
        "# plot_model(last_conv_layer_model)  # Uncomment to visualize the model"
      ],
      "metadata": {
        "id": "mUwDupeh5UZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# last_conv_layer_model.summary()"
      ],
      "metadata": {
        "id": "nDRbr73T6FET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reapplying the classifier on top of the last convolutional output\n",
        "\n",
        "# Create an input layer with the shape of the last convolutional layer's output\n",
        "classifier_input = keras.Input(shape=last_conv_layer.output.shape[1:])\n",
        "\n",
        "# Set the initial input tensor to the classifier_input\n",
        "x = classifier_input\n",
        "\n",
        "# Sequentially apply the classifier layers to the input (avg_pool and predictions layers)\n",
        "for layer_name in classifier_layer_names:\n",
        "    x = model.get_layer(layer_name)(x)  # Apply each layer from the original model\n",
        "\n",
        "# Create a new model that takes the classifier_input and outputs the final predictions after applying the classifier layers\n",
        "classifier_model = keras.Model(classifier_input, x)"
      ],
      "metadata": {
        "id": "VzruX5lB6Wwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieving the gradients of the top predicted class\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Use GradientTape to record the operations for automatic differentiation\n",
        "with tf.GradientTape() as tape:\n",
        "    # Pass the input image through the last convolutional layer model\n",
        "    last_conv_layer_output = last_conv_layer_model(img_array)\n",
        "\n",
        "    # Watch the output of the last convolutional layer to compute the gradient\n",
        "    tape.watch(last_conv_layer_output)\n",
        "\n",
        "    # Pass the convolutional layer output through the classifier model to get predictions\n",
        "    preds = classifier_model(last_conv_layer_output)\n",
        "\n",
        "    # Get the index of the top predicted class\n",
        "    top_pred_index = tf.argmax(preds[0])  # Find the index of the highest predicted class\n",
        "\n",
        "    # Extract the value of the top predicted class\n",
        "    top_class_channel = preds[:, top_pred_index]\n",
        "\n",
        "# Calculate the gradient of the top predicted class with respect to the output feature map of the last convolutional layer\n",
        "grads = tape.gradient(top_class_channel, last_conv_layer_output)  # This computes the gradient w.r.t. feature maps"
      ],
      "metadata": {
        "id": "qNOfPpMk9Cwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![picture](https://drive.google.com/uc?export=view&id=10QgdjWzPejhmCXLvzoXpf3_ySYx2BVCj)"
      ],
      "metadata": {
        "id": "eW-IgUQmP3O9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient pooling and channel-importance weighting\n",
        "\n",
        "# take an average of the gradients across all spatial dimensions (height, width, channels) to get channel importance weights\n",
        "pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2)).numpy()  # Computing the mean of gradients across the spatial dimensions (height, width) and channels\n",
        "\n",
        "# Convert the last convolutional layer output to a numpy array for further manipulation\n",
        "last_conv_layer_output = last_conv_layer_output.numpy()[0]  # We take the first (and only) image in the batch\n",
        "\n",
        "# Multiply each feature map by its corresponding importance weight (pooled gradients)\n",
        "for i in range(pooled_grads.shape[-1]):  # Loop through each channel (feature map)\n",
        "    last_conv_layer_output[:, :, i] *= pooled_grads[i]  # Element-wise multiplication of feature map with corresponding weight\n",
        "\n",
        "# Compute the weighted sum of the feature maps across all channels to generate the heatmap\n",
        "heatmap = np.mean(last_conv_layer_output, axis=-1)  # Average the weighted feature maps across all channels to get a 2D heatmap"
      ],
      "metadata": {
        "id": "g7gD_e-T9HDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Heatmap post-processing\n",
        "\n",
        "# Apply ReLU (Rectified Linear Unit) to the heatmap to remove negative values, as we are only interested in positive contributions\n",
        "heatmap = np.maximum(heatmap, 0)  # Negative values are replaced with 0, preserving only the positive areas that contributed to the prediction.\n",
        "\n",
        "# Normalize the heatmap by dividing by its maximum value so that the heatmap values are in the range [0, 1]\n",
        "heatmap /= np.max(heatmap)  # Scaling the heatmap so that the maximum value becomes 1\n",
        "\n",
        "# Display the heatmap using matplotlib\n",
        "plt.matshow(heatmap)  # Visualize the heatmap with a color map\n",
        "plt.axis('off')  # Hide axis labels for better visualization\n",
        "plt.show()  # Show the plot"
      ],
      "metadata": {
        "id": "Ff0Ii0Nq9Mmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Superimposing the heatmap on the original image\n",
        "\n",
        "import matplotlib.cm as cm  # Importing colormap utilities from matplotlib\n",
        "\n",
        "# Load the original image from the file path\n",
        "img = keras.utils.load_img(img_path)  # Load image from the given path\n",
        "img = keras.utils.img_to_array(img)  # Convert the image into a NumPy array for processing\n",
        "\n",
        "# Convert the heatmap values to the range [0, 255] for visualization\n",
        "heatmap = np.uint8(255 * heatmap)  # Scale heatmap values from [0, 1] to [0, 255]\n",
        "\n",
        "# Get the 'jet' colormap (a gradient from blue to red) to color the heatmap\n",
        "jet = cm.get_cmap(\"jet\")  # Get the 'jet' colormap from matplotlib\n",
        "jet_colors = jet(np.arange(256))[:, :3]  # Get RGB values (without alpha channel)\n",
        "jet_heatmap = jet_colors[heatmap]  # Map the heatmap values to the 'jet' colormap\n",
        "\n",
        "# Convert the heatmap to an image\n",
        "jet_heatmap = keras.utils.array_to_img(jet_heatmap)  # Convert array to a PIL image\n",
        "jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))  # Resize heatmap to match the original image dimensions\n",
        "jet_heatmap = keras.utils.img_to_array(jet_heatmap)  # Convert back to a NumPy array\n",
        "\n",
        "# Superimpose the heatmap on the original image with transparency\n",
        "superimposed_img = jet_heatmap * 0.4 + img  # Blend the heatmap and original image (0.4 transparency)\n",
        "superimposed_img = keras.utils.array_to_img(superimposed_img)  # Convert back to a PIL image\n",
        "\n",
        "# Save the resulting superimposed image to a file\n",
        "save_path = \"cat.jpg\"  # Path to save the superimposed image\n",
        "superimposed_img.save(save_path)  # Save the superimposed image to disk"
      ],
      "metadata": {
        "id": "keru57IC9PeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.matshow(superimposed_img)"
      ],
      "metadata": {
        "id": "sUWpJwxl9U28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see more gradCAM results\n",
        "\n",
        "Some good readings:\n",
        "* [blog](https://towardsdatascience.com/understand-your-algorithm-with-grad-cam-d3b62fce353#:~:text=Gradient%2Dweighted%20Class%20Activation%20Mapping,regions%20in%20the%20image%20for)"
      ],
      "metadata": {
        "id": "pxUW2yKcCyOF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Self-Practice Problem:\n",
        "Solve an image classification problem on the cats-vs-dogs dataset by training 'mini-Exception-like' model based on the instructions given below:\n",
        "\n",
        "\n",
        "1.  Set the global random seed to 42.\n",
        "2.  We are using a **cat-vs-dogs** dataset here. You will have to download it using instruction.\n",
        "\n",
        "  Download the data through the following command in your notebook\n",
        "\n",
        "`!wget -qq https://cdn.iisc.talentsprint.com/AIandMLOps/Datasets/cats_vs_dogs_small.zip`\n",
        "\n",
        "`!unzip -qq '/content/cats_vs_dogs_small.zip'`\n",
        "\n",
        "  Use the  image_dataset_from_directory utility from tensorflow.keras.utils to make appropriate datasets. (0)\n",
        "3.  Building the Model based on this [model_summary](https://indianinstituteofscience-my.sharepoint.com/:t:/g/personal/rohitc1_iisc_ac_in/EZ36t8eQFu9MrBnPjwueKcABD-2_8AZyDpyJ3vJEqUqlLQ?e=njNCGf) and its corresponding [model_plot](https://indianinstituteofscience-my.sharepoint.com/:i:/g/personal/rohitc1_iisc_ac_in/EU2WCnpqi8BEtfzltqI2vc4B5OFx53lMwn2tv6gqMebTig?e=lSsbz6). Ensure that you follow the trailing instructions:(16)\n",
        "\n",
        "      i).   For the initial layers of model mentioned in this [summary](https://indianinstituteofscience-my.sharepoint.com/:i:/g/personal/ksumanth_iisc_ac_in/EYDi7MfYgkRGvb-NqKKuGiABon9CyOUMEiffHac1sXyEsg?e=gYNdVB), random flip (horizontal), random rotation of 0.1, random zoom of 0.2, rescaling by 1./255, and set kernel_size = 5 and use_bias=False in the convolution layer.\n",
        "\n",
        "      ii).  Define a block of following layers:\n",
        "\n",
        "          *   Batch Normalization layer\n",
        "          *   Activation layer with relu as activation function\n",
        "          *   Depth wise separable layer (kernel size = 3)\n",
        "          *   Batch Normalization layer\n",
        "          *   Activation layer with relu as activation function\n",
        "          *   Depth wise separable layer (kernel size = 5)\n",
        "          *   Batch Normalization layer\n",
        "          *   Activation layer with relu as activation function\n",
        "          *   Depth wise separable layer (kernel size = 7)\n",
        "          *   Maxpool2D layer (poolsize =3, stride=2)\n",
        "          *   Convolution layer\n",
        "          *   'Add layer' due to a residual connection. Infer connection points of the skip connection from the model summary and model plot.\n",
        "\n",
        "          **Infer unspecified arguments from the summary**\n",
        "\n",
        "      iii). The block defined in (ii) repeats 4 times. Note that in each repitition, the number of filters changes. Infer this from the model plot/summary.\n",
        "\n",
        "      iv).  The last two layers are GlobalAveragePooling and Dense layers. The dense layer is the output layer (infer the number of neurons and the activation function).\n",
        "4. Compile model with rmsprop as an optimizer with appropriate loss and metric for this respective problem.\n",
        "5. Fit the model with a batch_size of 32 for 20 epochs. (Don't use EarlyStopping callback). Use the validation dataset from the data you downloaded. We have specified a small no. of epochs because training may take time. Try running colab on GPU by going to Edit > Notebook accelerator > Hardware Accelerator > GPU.\n",
        "6. Return the history as a DataFrame. Show loss and accuracy for training and validation through appropriate plots.\n",
        "7. Evaluate the Model on test dataset from the data you downloaded.\n",
        "\n",
        "**Note**:\n",
        "\n",
        "1. If you are using any parameter values or arguments apart from the ones mentioned or the ones that you must infer, state explicitly where and why you are using them.\n",
        "\n",
        "2. Also verify that the total no. of params of your model are the same as that mentioned in model_summary txt file given to you\n"
      ],
      "metadata": {
        "id": "PVfYIE6WqEcy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHfHdGCP_n6Y"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VgSwVENIPcM6"
      },
      "outputs": [],
      "source": [
        "#@title  Depth-wise separable convolution layer having 2 filters with size 3X3 is applied on an image of size 7X7X3. What is the count of trainable parameters in this layer? Consider all biases.{run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Answer = \"\" #@param [\"\", \"38\", \"33\", \"56\", \"60\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMzKSbLIgFzQ"
      },
      "outputs": [],
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjcH1VWSFI2l"
      },
      "outputs": [],
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"\" #@param {type:\"string\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VBk_4VTAxCM"
      },
      "outputs": [],
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XH91cL1JWH7m"
      },
      "outputs": [],
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8xLqj7VWIKW"
      },
      "outputs": [],
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FzAZHt1zw-Y-"
      },
      "outputs": [],
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_conv_output(n, p, f, s):\n",
        "    \"\"\"\n",
        "    Calculate the output shape of a convolution operation.\n",
        "\n",
        "    Parameters:\n",
        "    n (int): Size of the input (e.g., width or height of the input image).\n",
        "    p (int): Padding applied to the input.\n",
        "    f (int): Size of the convolutional filter (kernel size).\n",
        "    s (int): Stride of the convolution.\n",
        "\n",
        "    Returns:\n",
        "    int: Size of the output (width or height after the convolution).\n",
        "    \"\"\"\n",
        "    return ((n + 2 * p - f) // s) + 1\n",
        "\n",
        "# Example usage\n",
        "n = 6  # Size of the input\n",
        "p = 0  # Padding\n",
        "f = 3  # Filter size\n",
        "s = 1  # Stride\n",
        "\n",
        "output_size = calculate_conv_output(n, p, f, s)\n",
        "print(f\"The output size after the convolution is: {output_size}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6__wtKGx9iK",
        "outputId": "c2108288-ef5b-4c8d-8bad-69f418f5d40d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The output size after the convolution is: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_conv_params(f, n_c_prev, n_f):\n",
        "    \"\"\"\n",
        "    Calculate the number of parameters in a convolutional layer.\n",
        "\n",
        "    Parameters:\n",
        "    f (int): Size of the filter (kernel size, e.g., 3 for a 3x3 filter).\n",
        "    n_c_prev (int): Number of channels in the previous layer.\n",
        "    n_f (int): Number of filters in the current layer.\n",
        "\n",
        "    Returns:\n",
        "    int: Total number of parameters in the convolutional layer.\n",
        "    \"\"\"\n",
        "    # The formula: (f^2 * n_c_prev + 1) * n_f\n",
        "    return (f * f * n_c_prev + 1) * n_f\n",
        "\n",
        "# Example usage\n",
        "f = 3       # Filter size (e.g., 3x3 filter)\n",
        "n_c_prev = 3  # Number of channels in the previous layer\n",
        "n_f = 2       # Number of filters in the current layer\n",
        "\n",
        "total_params = calculate_conv_params(f, n_c_prev, n_f)\n",
        "print(f\"The total number of parameters in the convolutional layer is: {total_params}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAbfoENpz_jh",
        "outputId": "5fe4bd43-fd23-4c27-b512-72d5442004f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The total number of parameters in the convolutional layer is: 56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f= 3\n",
        "n_c_prev = 1\n",
        "n_f = 32\n",
        "total_params = calculate_conv_params(f, n_c_prev, n_f)\n",
        "print(f\"The total number of parameters in the convolutional layer is: {total_params}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dg1wuBvP06vd",
        "outputId": "d9ce53ee-7261-470d-e259-424c9a2bba24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The total number of parameters in the convolutional layer is: 320\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.datasets import mnist\n",
        "# Define the input layer with a shape of 28x28 pixels and 1 color channel (grayscale image)\n",
        "inputs = keras.Input(shape=(39, 39, 3))\n",
        "\n",
        "# First convolutional layer\n",
        "# - 32 filters, each of size 3x3\n",
        "# - ReLU activation function for non-linearity\n",
        "x = layers.Conv2D(filters=10, kernel_size=3, activation=\"relu\")(inputs)\n",
        "\n",
        "# Second convolutional layer\n",
        "# - 64 filters, each of size 3x3\n",
        "# - ReLU activation function\n",
        "x = layers.Conv2D(filters=20, kernel_size=5, activation=\"relu\", strides=2)(x)\n",
        "\n",
        "# Third convolutional layer\n",
        "# - 128 filters, each of size 3x3\n",
        "# - ReLU activation function\n",
        "x = layers.Conv2D(filters=40, kernel_size=5, activation=\"relu\", strides=2)(x)\n",
        "\n",
        "# Second pooling layer\n",
        "# - Max pooling with a 2x2 pool size, further reducing spatial dimensions by half (from 10x10 to 5x5)\n",
        "#x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "\n",
        "# Flatten the output of the last convolutional layer\n",
        "# This converts the 3D feature map into a 1D vector to be fed into the dense layer\n",
        "x = layers.Flatten()(x)\n",
        "\n",
        "# Output layer\n",
        "# - 10 units for 10 classes (e.g., digits 0–9 in a digit classification task)\n",
        "# - Softmax activation function for multi-class classification\n",
        "outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
        "\n",
        "# Create the model\n",
        "model_no_max_pool = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "Wp3vZX8WC6fz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print a summary of the model's architecture\n",
        "model_no_max_pool.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "YpWeDwbxE137",
        "outputId": "2b49953a-e8d3-4975-cb2f-4e4749f5b2c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_4 (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m39\u001b[0m, \u001b[38;5;34m39\u001b[0m, \u001b[38;5;34m3\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_10 (\u001b[38;5;33mConv2D\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m37\u001b[0m, \u001b[38;5;34m37\u001b[0m, \u001b[38;5;34m10\u001b[0m)          │             \u001b[38;5;34m280\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_11 (\u001b[38;5;33mConv2D\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m20\u001b[0m)          │           \u001b[38;5;34m5,020\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_12 (\u001b[38;5;33mConv2D\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m40\u001b[0m)            │          \u001b[38;5;34m20,040\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten_2 (\u001b[38;5;33mFlatten\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1960\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │          \u001b[38;5;34m19,610\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">39</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">39</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">280</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">5,020</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">20,040</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1960</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">19,610</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m44,950\u001b[0m (175.59 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">44,950</span> (175.59 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m44,950\u001b[0m (175.59 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">44,950</span> (175.59 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}