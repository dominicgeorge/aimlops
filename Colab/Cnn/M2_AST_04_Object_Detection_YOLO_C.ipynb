{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","metadata":{"id":"hH3UvBtnW755"},"source":["# Advanced Certification Programme in AI and MLOps\n","## A programme by IISc and TalentSprint\n","### Assignment: Object Detection"]},{"cell_type":"markdown","metadata":{"id":"KXubZhEt6g3u"},"source":["## Learning Objectives\n","\n","At the end of the experiment you will be able to :\n","\n","* understand what is object detection\n","* configure a state-of-the-art algorithm called YOLO V3\n","* make inferences from YOLO on real-time images"]},{"cell_type":"markdown","source":["## Introduction:\n"],"metadata":{"id":"3dOcjTZXAYX_"}},{"cell_type":"markdown","source":["### What is YOLO V3?\n","YOLOv3 (You Only Look Once, Version 3) is a real-time object detection algorithm that identifies specific objects in videos, live feeds or images. The YOLO machine learning algorithm uses features learned by a deep convolutional neural network to detect an object. YOLO has the advantage of being much faster than other networks and still maintains accuracy.\n","\n","The approach involves a single deep convolutional neural network (originally a version of GoogLeNet, later updated and called **DarkNet** based on VGG) that splits the input into a grid of cells and each cell directly predicts a bounding box and object classification. The result is a large number of candidate bounding boxes that are consolidated into a final prediction by a post-processing step.\n","\n","It allows the model to look at the whole image at test time, so its predictions are informed by the global context in the image. Using CNN, YOLO can predict all objects in one forward pass and that is the reason for its full name “You Only Look Once”. You Only Look Once means it looks for the image/frame only once and is able to detect all the objects in the image/frame.\n","\n","High-scoring regions are noted as positive detections of whatever class they most closely identify with. For example, in a live feed of traffic, YOLO can be used to detect different kinds of vehicles depending on which regions of the video score highly in comparison to predefined classes of vehicles.\n","\n"],"metadata":{"id":"IhKMwE8CTgBY"}},{"cell_type":"markdown","source":["### How YOLOv3 works?\n","\n","The YOLOv3 network divides an input image into an S x S grid of cells and predicts bounding boxes as well as class probabilities for each grid. Each grid cell is responsible for predicting B-bounding boxes and C-class probabilities of objects whose centers fall inside the grid cell. Bounding boxes are the regions of interest (ROI) of the candidate objects. The “B” is associated with the number of using anchors. Each bounding box has (5 + C) attributes. The value of “5” is related to 5 bounding box attributes, which are center coordinates (bx, by) and shape (bh, bw) of the bounding box, and one confidence score. The “C” is the number of classes. The confidence score reflects how confident a box contains an object. The confidence score is in the range of 0 – 1.\n","\n","Since we have an S x S grid of cells, after running a single forward pass convolutional neural network to the whole image, YOLOv3 produces a 3-D tensor with the shape of [S, S, B * (5 + C].\n","\n","The following figure illustrates the basic principle of YOLOv3 where the input image is divided into the 13 x 13 grid of cells (13 x 13 grid of cells is used for the first scale, whereas YOLOv3 actually uses 3 different scales and we're going to discuss it in the section prediction across scale).\n","\n","<center>\n","<img style=\"-webkit-user-select: none;margin: auto;\" src=\"https://machinelearningspace.com/wp-content/uploads/2020/01/bbox_ok-2-868x1024.png\" width=\"800\" height=\"800\">\n","</center>\n","\n","YOLOv3 was trained on the COCO dataset with C=80(class) and B=3(). So, for the first prediction scale, after a single forward pass of CNN, the YOLOv3 outputs a tensor with the shape of [(13, 13, 3 * (5 + 80)]."],"metadata":{"id":"TeaQBRmj4rLF"}},{"cell_type":"markdown","metadata":{"id":"BNLA8HiKxQhc"},"source":["### Setup Steps:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2YzfoPvJDiTX"},"outputs":[],"source":["#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n","Id = \"\" #@param {type:\"string\"}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AjoZJWGErxGf"},"outputs":[],"source":["#@title Please enter your password (your registered phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n","password = \"\" #@param {type:\"string\"}"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"WBPPuGmBlDIN","collapsed":true},"outputs":[],"source":["#@title Run this cell to complete the setup for this Notebook\n","from IPython import get_ipython\n","\n","ipython = get_ipython()\n","\n","notebook= \"M2_AST_04_Object_Detection_YOLO_C\" #name of the notebook\n","\n","def setup():\n","#  ipython.magic(\"sx pip3 install torch\")\n","\n","    # ipython.magic(\"wget https://cdn.iisc.talentsprint.com/AIandMLOps/Datasets/Acoustic_Extinguisher_Fire_Dataset.xlsx\")\n","    from IPython.display import HTML, display\n","    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n","    print(\"Setup completed successfully\")\n","    return\n","\n","def submit_notebook():\n","    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n","\n","    import requests, json, base64, datetime\n","\n","    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n","    if not submission_id:\n","      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","\n","      if r[\"status\"] == \"Success\":\n","          return r[\"record_id\"]\n","      elif \"err\" in r:\n","        print(r[\"err\"])\n","        return None\n","      else:\n","        print (\"Something is wrong, the notebook will not be submitted for grading\")\n","        return None\n","\n","    elif getAnswer() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n","      f = open(notebook + \".ipynb\", \"rb\")\n","      file_hash = base64.b64encode(f.read())\n","\n","      data = {\"complexity\" : Complexity, \"additional\" :Additional,\n","              \"concepts\" : Concepts, \"record_id\" : submission_id,\n","              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n","              \"notebook\" : notebook,\n","              \"feedback_experiments_input\" : Comments,\n","              \"feedback_mentor_support\": Mentor_support}\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","      if \"err\" in r:\n","        print(r[\"err\"])\n","        return None\n","      else:\n","        print(\"Your submission is successful.\")\n","        print(\"Ref Id:\", submission_id)\n","        print(\"Date of submission: \", r[\"date\"])\n","        print(\"Time of submission: \", r[\"time\"])\n","        print(\"View your submissions: https://aimlops-iisc.talentsprint.com/notebook_submissions\")\n","        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n","        return submission_id\n","    else: submission_id\n","\n","\n","def getAdditional():\n","  try:\n","    if not Additional:\n","      raise NameError\n","    else:\n","      return Additional\n","  except NameError:\n","    print (\"Please answer Additional Question\")\n","    return None\n","\n","def getComplexity():\n","  try:\n","    if not Complexity:\n","      raise NameError\n","    else:\n","      return Complexity\n","  except NameError:\n","    print (\"Please answer Complexity Question\")\n","    return None\n","\n","def getConcepts():\n","  try:\n","    if not Concepts:\n","      raise NameError\n","    else:\n","      return Concepts\n","  except NameError:\n","    print (\"Please answer Concepts Question\")\n","    return None\n","\n","\n","# def getWalkthrough():\n","#   try:\n","#     if not Walkthrough:\n","#       raise NameError\n","#     else:\n","#       return Walkthrough\n","#   except NameError:\n","#     print (\"Please answer Walkthrough Question\")\n","#     return None\n","\n","def getComments():\n","  try:\n","    if not Comments:\n","      raise NameError\n","    else:\n","      return Comments\n","  except NameError:\n","    print (\"Please answer Comments Question\")\n","    return None\n","\n","\n","def getMentorSupport():\n","  try:\n","    if not Mentor_support:\n","      raise NameError\n","    else:\n","      return Mentor_support\n","  except NameError:\n","    print (\"Please answer Mentor support Question\")\n","    return None\n","\n","def getAnswer():\n","  try:\n","    if not Answer:\n","      raise NameError\n","    else:\n","      return Answer\n","  except NameError:\n","    print (\"Please answer Question\")\n","    return None\n","\n","\n","def getId():\n","  try:\n","    return Id if Id else None\n","  except NameError:\n","    return None\n","\n","def getPassword():\n","  try:\n","    return password if password else None\n","  except NameError:\n","    return None\n","\n","submission_id = None\n","### Setup\n","if getPassword() and getId():\n","  submission_id = submit_notebook()\n","  if submission_id:\n","    setup()\n","else:\n","  print (\"Please complete Id and Password cells before running setup\")\n","\n"]},{"cell_type":"markdown","source":["### Importing required packages"],"metadata":{"id":"ppHIOtOc-Nt_"}},{"cell_type":"code","source":["import os  # Provides functions to interact with the operating system, such as file and directory manipulation\n","import numpy as np  # A library for numerical computations, particularly for handling arrays and matrices\n","from keras.layers import Conv2D, Input, BatchNormalization, LeakyReLU, ZeroPadding2D, UpSampling2D\n","# Imports common neural network layers from Keras:\n","# - Conv2D: 2D convolution layer, useful for feature extraction\n","# - Input: Specifies the input tensor for the model\n","# - BatchNormalization: Normalizes inputs to stabilize and accelerate training\n","# - LeakyReLU: Activation function allowing a small gradient for negative inputs\n","# - ZeroPadding2D: Adds padding of zeros to the input image\n","# - UpSampling2D: Upsamples the input to a higher resolution\n","\n","from keras.layers import add, concatenate\n","# - add: Adds two tensors element-wise\n","# - concatenate: Joins two tensors along a specific axis\n","\n","from keras.models import Model\n","# Allows building models by specifying the input and output layers\n","\n","import struct  # Handles binary data and performs conversions between Python objects and C structs\n","import cv2  # OpenCV library for image processing and computer vision tasks\n","import keras  # High-level API for building and training neural networks, running on top of TensorFlow"],"metadata":{"id":"-afgYAC4-RXg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Pre-trained Weights from the repository\n","\n","Model Weights can be downloaded from the website of the original creator of YOLOv3. The model has already been trained by the authors and thus it is easier to use the pre-trained weights for inference. Download the model weights and place them into your current directory with the filename `“yolov3.weights.”`. These were trained using the DarkNet code base on the MSCOCO dataset.\n","\n","**Note:** Refer to the following [link](https://pjreddie.com/darknet/yolo/)"],"metadata":{"id":"YHosH2BRSCr1"}},{"cell_type":"code","source":["#@title Download the pre-trained weight\n","!wget -qq https://pjreddie.com/media/files/yolov3.weights"],"metadata":{"id":"rAvba_LTUOBw","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Create a class WeightReader to load the pre-trained weights for yolov3\n","\n","WeightReader class will parse the file and load the model weights into memory to set it in our Keras model.\n","\n","We will be reading the original weights of Yolo. The original network was not trained in Keras and thus we define a special class to read weights."],"metadata":{"id":"LsdWDGyp-sPK"}},{"cell_type":"code","source":["# Class to load pretrained weights into a model\n","class WeightReader:\n","    def __init__(self, weight_file):\n","        # Open the weight file in binary mode for reading\n","        with open(weight_file, 'rb') as w_f:\n","            # Read and unpack the file header (major, minor, and revision numbers)\n","            major,    = struct.unpack('i', w_f.read(4))  # Read 4 bytes as an integer (major version)\n","            minor,    = struct.unpack('i', w_f.read(4))  # Read 4 bytes as an integer (minor version)\n","            revision, = struct.unpack('i', w_f.read(4))  # Read 4 bytes as an integer (revision number)\n","\n","            # Skip extra header bytes depending on the version\n","            if (major * 10 + minor) >= 2 and major < 1000 and minor < 1000:\n","                w_f.read(8)  # Skip 8 bytes for newer formats\n","            else:\n","                w_f.read(4)  # Skip 4 bytes for older formats\n","\n","            # Determine whether weights are transposed based on the version\n","            transpose = (major > 1000) or (minor > 1000)\n","\n","            # Read the rest of the file as binary data\n","            binary = w_f.read()\n","\n","        # Initialize offset for reading weights and store all weights in a NumPy array\n","        self.offset = 0\n","        self.all_weights = np.frombuffer(binary, dtype='float32')  # Convert binary data to a float32 array\n","\n","    def read_bytes(self, size):\n","        # Reads 'size' number of weights from the current offset\n","        self.offset = self.offset + size  # Update the offset\n","        return self.all_weights[self.offset-size:self.offset]  # Return the weights read\n","\n","    # Load weights into the provided Keras model\n","    def load_weights(self, model):\n","        # Loop through 106 layers, assuming a specific architecture\n","        for i in range(106):\n","            try:\n","                # Get the convolutional layer by name\n","                conv_layer = model.get_layer('conv_' + str(i))\n","                print(\"loading weights of convolution #\" + str(i))\n","\n","                # Skip layers 81, 93, and 105 for normalization\n","                if i not in [81, 93, 105]:\n","                    # Get the batch normalization layer associated with the convolutional layer\n","                    norm_layer = model.get_layer('bnorm_' + str(i))\n","\n","                    # Get the size of weights for the normalization layer\n","                    size = np.prod(norm_layer.get_weights()[0].shape)\n","\n","                    # Read normalization parameters: beta, gamma, mean, and variance\n","                    beta  = self.read_bytes(size)  # Offset (bias)\n","                    gamma = self.read_bytes(size)  # Scale\n","                    mean  = self.read_bytes(size)  # Mean\n","                    var   = self.read_bytes(size)  # Variance\n","\n","                    # Set the weights for the normalization layer\n","                    norm_layer.set_weights([gamma, beta, mean, var])\n","\n","                # Check if the convolutional layer has biases\n","                if len(conv_layer.get_weights()) > 1:\n","                    # Read the bias and kernel weights\n","                    bias   = self.read_bytes(np.prod(conv_layer.get_weights()[1].shape))\n","                    kernel = self.read_bytes(np.prod(conv_layer.get_weights()[0].shape))\n","\n","                    # Reshape and transpose the kernel weights to match the layer's expected shape\n","                    kernel = kernel.reshape(list(reversed(conv_layer.get_weights()[0].shape)))\n","                    kernel = kernel.transpose([2, 3, 1, 0])  # Transpose to Keras format\n","\n","                    # Set the weights for the convolutional layer\n","                    conv_layer.set_weights([kernel, bias])\n","                else:\n","                    # For layers without biases, only set the kernel weights\n","                    kernel = self.read_bytes(np.prod(conv_layer.get_weights()[0].shape))\n","                    kernel = kernel.reshape(list(reversed(conv_layer.get_weights()[0].shape)))\n","                    kernel = kernel.transpose([2, 3, 1, 0])\n","                    conv_layer.set_weights([kernel])\n","            except ValueError:\n","                # If the layer is not found in the model, print a message\n","                print(\"no convolution #\" + str(i))\n","\n","    def reset(self):\n","        # Resets the offset to the beginning for re-reading weights\n","        self.offset = 0\n"],"metadata":{"id":"eFteK_gv-2fv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### YOLO V3 Architecture\n","\n","YOLO v3 is using a Darknet-53  network to perform feature extraction which is undeniably larger compare to YOLO v2. This network is composed of 53 convolutional layers with shortcut connections (Redmon & Farhadi, 2018).\n","\n","The code implementation is composed of several components as given below:\n","\n","<center>\n","<img style=\"-webkit-user-select: none;margin: auto;\" src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*-Whes2CytS_v22Wx1LZ6RA.png\" width=\"500\n","\" height=\"700\">\n","</center>\n","\n","The 53 layers of the darknet are further stacked with 53 more layers for the detection head, making YOLO v3 a total of a 106-layer fully convolutional underlying architecture. This leading to a large architecture, though making it a bit slower as compared to YOLO v2, but enhancing the accuracy at the same time.\n","\n","\n","The diagram given below explains the complete architecture of YOLO v3 (Combining both, the extractor and the detector). [ [Reference](https://towardsdatascience.com/yolo-v3-object-detection-53fb7d3bfe6b) ]\n","\n","\n","\n","<center>\n","<img style=\"-webkit-user-select: none;margin: auto;\" src=\"https://miro.medium.com/v2/resize:fit:2000/format:webp/1*d4Eg17IVJ0L41e7CTWLLSg.png\" width=\"1600\n","\" height=\"600\">\n","</center>"],"metadata":{"id":"xoA3Y_qiIg3Q"}},{"cell_type":"markdown","source":["### Create the Yolo v3 model\n","\n","We first create a function for creating the Convolutional blocks and then define the blocks required for the YOLO model.\n","\n","* _conv_block function which is used to construct a convolutional layer\n","* make_yolov3_model function which is used to create layers of convolutional and stack together as a whole."],"metadata":{"id":"b1QBN-CJAlfk"}},{"cell_type":"code","source":["# Defines a convolutional block with optional skip connections\n","def _conv_block(inp, convs, skip=True):\n","    \"\"\"\n","    Parameters:\n","    - inp: Input tensor to the block.\n","    - convs: A list of dictionaries, where each dictionary contains configuration\n","             for a convolutional layer (e.g., filter size, kernel size, stride, etc.).\n","    - skip: Boolean indicating whether to add a skip connection (residual connection).\n","\n","    Returns:\n","    - Output tensor after applying the convolutional block.\n","    \"\"\"\n","    x = inp  # Initialize the output tensor with the input tensor\n","    count = 0  # Counter to track the current convolution layer index in the loop\n","\n","    for conv in convs:\n","        # Save the current tensor as a skip connection before the second-to-last convolution, if skip is enabled\n","        if count == (len(convs) - 2) and skip:\n","            skip_connection = x  # Save tensor for the residual connection\n","        count += 1  # Increment the counter\n","\n","        # Apply zero padding for convolutions with stride > 1\n","        if conv['stride'] > 1:\n","            x = ZeroPadding2D(((1, 0), (1, 0)))(x)  # Peculiar padding, as Darknet prefers padding on the left and top\n","\n","        # Apply a convolutional layer\n","        x = Conv2D(conv['filter'],  # Number of filters (output channels)\n","                   conv['kernel'],  # Kernel size\n","                   strides=conv['stride'],  # Stride of the convolution\n","                   padding='valid' if conv['stride'] > 1 else 'same',  # Valid padding for strided conv, otherwise same\n","                   name='conv_' + str(conv['layer_idx']),  # Unique name for the layer\n","                   use_bias=False if conv['bnorm'] else True)(x)  # Use bias only if batch normalization is not applied\n","\n","        # Apply batch normalization if specified\n","        if conv['bnorm']:\n","            x = BatchNormalization(epsilon=0.001, name='bnorm_' + str(conv['layer_idx']))(x)\n","\n","        # Apply Leaky ReLU activation if specified\n","        if conv['leaky']:\n","            x = LeakyReLU(alpha=0.1, name='leaky_' + str(conv['layer_idx']))(x)\n","\n","    # Add skip connection (residual connection) if specified\n","    return add([skip_connection, x]) if skip else x\n"],"metadata":{"id":"2CvGDVjDa_VT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next, we create a Darknet with 108 convolutional layers."],"metadata":{"id":"FgjM81XrE3MJ"}},{"cell_type":"code","source":["def make_yolov3_model():\n","    # Define the input layer with dynamic shape for width and height and 3 channels (RGB image).\n","    input_image = Input(shape=(None, None, 3))\n","\n","    # Layer 0 => 4: Initial convolutional blocks for feature extraction.\n","    # This creates two convolutional layers with increasing filters (32, 64) and strides, followed by two layers for refinement.\n","    x = _conv_block(input_image, [{'filter': 32, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 0},\n","                                  {'filter': 64, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 1},\n","                                  {'filter': 32, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 2},\n","                                  {'filter': 64, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 3}])\n","\n","    # Layer 5 => 8: Transition to higher-level features by increasing filter size to 128.\n","    x = _conv_block(x, [{'filter': 128, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 5},\n","                        {'filter':  64, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 6},\n","                        {'filter': 128, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 7}])\n","\n","    # Layer 9 => 11: Refinement of 128-filter block features.\n","    x = _conv_block(x, [{'filter':  64, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 9},\n","                        {'filter': 128, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 10}])\n","\n","    # Layer 12 => 15: Transition to 256 filters for capturing more complex features.\n","    x = _conv_block(x, [{'filter': 256, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 12},\n","                        {'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 13},\n","                        {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 14}])\n","\n","    # Layer 16 => 36: Residual blocks with 256 filters, repeated 7 times for feature enhancement.\n","    for i in range(7):\n","        x = _conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 16+i*3},\n","                            {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 17+i*3}])\n","\n","    # Save the output for later skip connection.\n","    skip_36 = x\n","\n","    # Layer 37 => 40: Transition to 512 filters for higher-level features.\n","    x = _conv_block(x, [{'filter': 512, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 37},\n","                        {'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 38},\n","                        {'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 39}])\n","\n","    # Layer 41 => 61: Residual blocks with 512 filters, repeated 7 times.\n","    for i in range(7):\n","        x = _conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 41+i*3},\n","                            {'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 42+i*3}])\n","\n","    # Save the output for later skip connection.\n","    skip_61 = x\n","\n","    # Layer 62 => 65: Transition to 1024 filters for the highest-level features.\n","    x = _conv_block(x, [{'filter': 1024, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 62},\n","                        {'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 63},\n","                        {'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 64}])\n","\n","    # Layer 66 => 74: Refinement of the 1024-filter block, repeated 3 times.\n","    for i in range(3):\n","        x = _conv_block(x, [{'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 66+i*3},\n","                            {'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 67+i*3}])\n","\n","    # Layer 75 => 79: Final refinement before the first YOLO output.\n","    x = _conv_block(x, [{'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 75},\n","                        {'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 76},\n","                        {'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 77},\n","                        {'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 78},\n","                        {'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 79}], skip=False)\n","\n","    # Layer 80 => 82: YOLO output 1 (large objects).\n","    yolo_82 = _conv_block(x, [{'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 80},\n","                              {'filter':  255, 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 81}], skip=False)\n","\n","    # Layer 83 => 86: Upsampling and skip connection with `skip_61` for medium objects.\n","    x = _conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 84}], skip=False)\n","    x = UpSampling2D(2)(x)\n","    x = concatenate([x, skip_61])\n","\n","    # Layer 87 => 91: Refinement of medium-level features.\n","    x = _conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 87},\n","                        {'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 88},\n","                        {'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 89},\n","                        {'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 90},\n","                        {'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 91}], skip=False)\n","\n","    # Layer 92 => 94\n","    # This block processes the feature map for medium-sized object detection.\n","    # Layer 92 applies a 3x3 convolution with 512 filters, batch normalization, and a leaky ReLU activation to refine features.\n","    # Layer 93 applies a 1x1 convolution with 255 filters (corresponding to 3 anchor boxes × (5 bounding box attributes + number of classes)).\n","    # No batch normalization or activation is applied in Layer 93 since it prepares the final prediction output for this scale.\n","    yolo_94 = _conv_block(x, [{'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 92},\n","                              {'filter': 255, 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 93}], skip=False)\n","\n","    # Layer 95 => 98\n","    # This section upsamples the feature map and combines it with earlier feature maps for detecting smaller objects.\n","    # Layer 95 reduces the depth of the feature map using a 1x1 convolution with 128 filters while retaining key information.\n","    x = _conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True,   'layer_idx': 96}], skip=False)\n","\n","    # The UpSampling2D layer scales the feature map by a factor of 2, effectively doubling its spatial resolution.\n","    x = UpSampling2D(2)(x)\n","\n","    # The feature map is concatenated with the feature map from layer 36 (skip_36) to combine higher-resolution details for small object detection.\n","    x = concatenate([x, skip_36])\n","\n","    # Layer 99 => 106\n","    # This block generates the final detection feature map for small objects.\n","    # Alternating 1x1 and 3x3 convolutions refine the feature map, focusing on small-scale object features.\n","    # - Layers 99, 101, and 103 apply 1x1 convolutions with 128 filters to reduce dimensionality while retaining key features.\n","    # - Layers 100, 102, and 104 apply 3x3 convolutions with 256 filters to extract spatial patterns.\n","    # Layer 105 performs a 1x1 convolution with 255 filters, similar to Layer 93, to prepare the output for detection.\n","    # This prepares predictions for the third detection scale, focusing on small objects.\n","    yolo_106 = _conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 99},\n","                              {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 100},\n","                              {'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 101},\n","                              {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 102},\n","                              {'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 103},\n","                              {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 104},\n","                              {'filter': 255, 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 105}], skip=False)\n","\n","    # Model definition\n","    # Combines three outputs for multi-scale object detection:\n","    # - yolo_82 for large objects.\n","    # - yolo_94 for medium objects.\n","    # - yolo_106 for small objects.\n","    # Returns a Keras model capable of detecting objects at three different scales.\n","    model = Model(input_image, [yolo_82, yolo_94, yolo_106])\n","    return model"],"metadata":{"id":"j1KYQCP-EqNB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Define the YOLO v3 model & setting the weights\n","\n","Load the pre-trained weights which we downloaded earlier. Save the model using Keras save function and specifying the filename."],"metadata":{"id":"SAkPff7KJH4V"}},{"cell_type":"code","source":["# define the yolo v3 model\n","yolov3 = make_yolov3_model()\n","\n","# load the weights\n","weight_reader = WeightReader('yolov3.weights')\n","\n","# set the weights\n","weight_reader.load_weights(yolov3)\n","\n","# save the model to file\n","# yolov3.save('model.h5')"],"metadata":{"id":"Kz1oBAyNIk6Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["yolov3.summary()"],"metadata":{"id":"nr7bZqBoIer6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Download a sample image from the Web, or upload your image to pass through the model to get a prediction."],"metadata":{"id":"DknvBxYdJa7f"}},{"cell_type":"code","source":["#@title Download sample images\n","!wget -qq \"https://thumbs.dreamstime.com/b/golden-retriever-dog-21668976.jpg\" -O dog.jpg\n","!wget -qq \"https://cdn.extras.talentsprint.com/DLFA/Experiment_related_data/bus.jpg\""],"metadata":{"id":"1p5CPRqQ6er5","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image_path = \"/content/bus.jpg\"\n","#image_path = \"/content/dog.jpg\""],"metadata":{"id":"drk9n7tlKRiF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Preprocessing the Input Image\n","\n","We need to preprocess the image before passing through the network to ensure the shape of the image changes uniformly throughout the network."],"metadata":{"id":"N4-eTxgTJ1_i"}},{"cell_type":"code","source":["def preprocess_input(image, net_h, net_w):\n","    \"\"\"\n","    Preprocess an input image for a neural network:\n","    - Resize the image while maintaining the aspect ratio.\n","    - Embed the resized image into a fixed-sized \"letterbox\" frame.\n","\n","    Parameters:\n","    - image: Input image as a NumPy array (height x width x 3).\n","    - net_h, net_w: Target height and width for the network input.\n","\n","    Returns:\n","    - new_image: Preprocessed image as a NumPy array (1 x net_h x net_w x 3).\n","    \"\"\"\n","    # Get the dimensions of the input image\n","    new_h, new_w, _ = image.shape\n","\n","    # Determine the new size of the image while maintaining the aspect ratio\n","    if (float(net_w) / new_w) < (float(net_h) / new_h):  # Check if width scaling is the limiting factor\n","        new_h = (new_h * net_w) / new_w  # Scale the height proportionally to fit the width\n","        new_w = net_w  # Set the new width to the network width\n","    else:  # Height scaling is the limiting factor\n","        new_w = (new_w * net_h) / new_h  # Scale the width proportionally to fit the height\n","        new_h = net_h  # Set the new height to the network height\n","\n","    # Resize the image to the computed new size\n","    # - Convert BGR to RGB using `[:,:,::-1]`\n","    # - Normalize pixel values to the range [0, 1] by dividing by 255\n","    resized = cv2.resize(image[:, :, ::-1] / 255., (int(new_w), int(new_h)))\n","\n","    # Create a \"letterbox\" image filled with a gray background (0.5 in normalized range)\n","    new_image = np.ones((net_h, net_w, 3)) * 0.5\n","\n","    # Embed the resized image into the center of the letterbox\n","    # - Calculate top-left corner for centering\n","    new_image[int((net_h - new_h) // 2):int((net_h + new_h) // 2),\n","              int((net_w - new_w) // 2):int((net_w + new_w) // 2), :] = resized\n","\n","    # Add a batch dimension to the image (required for neural network input)\n","    new_image = np.expand_dims(new_image, 0)\n","\n","    return new_image\n"],"metadata":{"id":"hUwX8cWFJ3KS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Reading the image from the path and applying the pre-processing function:"],"metadata":{"id":"GIbqnnpZKkmP"}},{"cell_type":"code","source":["# Set the target input size for Yolov3 model\n","# Yolov3 expects an input image of size 416x416, so we set the target height and width.\n","net_h, net_w = 416, 416\n","\n","# Read the input image from the given file path\n","image = cv2.imread(image_path)  # 'image_path' is the file location of the image\n","\n","# Get the dimensions (height, width, channels) of the input image\n","image_h, image_w, _ = image.shape  # 'image.shape' returns a tuple (height, width, channels)\n","\n","# Preprocess the image to match the expected input size for Yolov3 (416x416)\n","# This function might resize the image, normalize pixel values, or apply other transformations.\n","new_image = preprocess_input(image, net_h, net_w)\n","\n","# Output the shape of the pre-processed image\n","# The shape of 'new_image' should now be (416, 416, channels) as per the Yolov3 input requirements.\n","new_image.shape"],"metadata":{"id":"CHeAeORqKkAI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Getting prediction output from the Yolo network"],"metadata":{"id":"EQlxKFADLIVr"}},{"cell_type":"code","source":["# Make a prediction using the Yolov3 model\n","# The 'predict' method generates predictions based on the pre-processed image input (new_image).\n","y_pred = yolov3.predict(new_image)  # 'new_image' is the pre-processed image tensor ready for prediction\n","\n","# Summarize the shape of each element in the prediction output\n","# Yolov3 typically returns a list of arrays, one for each detection layer, each containing the predicted bounding boxes, class probabilities, and confidence scores.\n","print([a.shape for a in y_pred])  # Print the shape of each element in 'y_pred' to understand the output structure"],"metadata":{"id":"JKqVefcyLMrT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Decoding the output into boxes\n","\n","The output of the YOLO v3 prediction is in the form of a list of arrays that are hard to be interpreted. As YOLO v3 is a multi-scale detection, it is decoded into three different scales in the shape of (13, 13, 225), (26, 26, 225), and (52, 52, 225).\n","\n","We need to create a few helping classes & functions to get output as a bounding box and prediction."],"metadata":{"id":"SnmYzdYAME1C"}},{"cell_type":"markdown","source":["### Create a class for the Bounding Box\n","\n","BoundBox defines the corners of each bounding box in the context of the input image shape and class probabilities."],"metadata":{"id":"_vDRxmczKUAH"}},{"cell_type":"code","source":["class BoundBox:\n","    # Initialize the BoundBox object with the coordinates of the bounding box\n","    # and optional objectness and class probabilities\n","    def __init__(self, xmin, ymin, xmax, ymax, objness=None, classes=None):\n","        self.xmin = xmin  # Minimum x-coordinate (left side) of the bounding box\n","        self.ymin = ymin  # Minimum y-coordinate (top side) of the bounding box\n","        self.xmax = xmax  # Maximum x-coordinate (right side) of the bounding box\n","        self.ymax = ymax  # Maximum y-coordinate (bottom side) of the bounding box\n","\n","        self.objness = objness  # Confidence score that indicates the presence of an object in the box (optional)\n","        self.classes = classes  # Probability distribution over different classes (optional)\n","\n","        self.label = -1  # Initialize label to -1; it will store the class with the highest probability\n","        self.score = -1  # Initialize score to -1; it will store the highest probability score for the label\n","\n","    # Method to get the label of the object inside the bounding box\n","    def get_label(self):\n","        # If label is not yet assigned (still -1), compute the class with the highest probability\n","        if self.label == -1:\n","            self.label = np.argmax(self.classes)  # Label is the class with the highest probability\n","        return self.label  # Return the assigned label\n","\n","    # Method to get the score (probability) associated with the label\n","    def get_score(self):\n","        # If score is not yet assigned (still -1), compute the score based on the label\n","        if self.score == -1:\n","            self.score = self.classes[self.get_label()]  # The score is the probability for the most likely class\n","        return self.score  # Return the score associated with the label\n"],"metadata":{"id":"7vLaBRSZ_skW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Intersection over Union(IoU) calculation\n","\n","The following functions help in calculating IoU:\n","\n","* Sigmoid function\n","* _interval_overlap : Checks if two intervals overlap. Two intervals do not overlap when one ends before the other begins.\n","* bbox_iou : Calculates intersection over union(IoU) of two boxes.\n","\n","[Reference: IoU](https://learnopencv.com/intersection-over-union-iou-in-object-detection-and-segmentation/)"],"metadata":{"id":"-r-eCsSGbBYV"}},{"cell_type":"code","source":["def _sigmoid(x):\n","    # Apply the sigmoid function element-wise to the input x.\n","    # The sigmoid function maps any input value to a value between 0 and 1.\n","    # It is commonly used for activation functions in neural networks and for probabilities.\n","    return 1. / (1. + np.exp(-x))  # Return the sigmoid of x using the formula: 1 / (1 + exp(-x))"],"metadata":{"id":"LG4Mpcm6b4JN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Helper function to calculate the overlap between two intervals\n","def _interval_overlap(interval_a, interval_b):\n","    # Extract the start and end points for each interval\n","    x1, x2 = interval_a  # Interval A: [x1, x2]\n","    x3, x4 = interval_b  # Interval B: [x3, x4]\n","\n","    # Check if the intervals do not overlap\n","    if x3 < x1:\n","        # If interval B ends before interval A starts, there's no overlap\n","        if x4 < x1:\n","            return 0\n","        else:\n","            # Otherwise, there's partial overlap: the overlap is from x1 to the minimum of x2 and x4\n","            return min(x2, x4) - x1\n","    else:\n","        # Case when interval B starts after interval A ends, check for overlap\n","        if x2 < x3:\n","            # No overlap if interval A ends before interval B starts\n","            return 0\n","        else:\n","            # Otherwise, there's partial overlap: the overlap is from x3 to the minimum of x2 and x4\n","            return min(x2, x4) - x3\n","\n","# Intersection over Union (IoU) function to measure the overlap between two bounding boxes\n","# IOU = Area of Intersection / Area of Union\n","def bbox_iou(box1, box2):\n","    # Calculate the width of the intersection between box1 and box2 along the x-axis\n","    intersect_w = _interval_overlap([box1.xmin, box1.xmax], [box2.xmin, box2.xmax])\n","\n","    # Calculate the height of the intersection between box1 and box2 along the y-axis\n","    intersect_h = _interval_overlap([box1.ymin, box1.ymax], [box2.ymin, box2.ymax])\n","\n","    # Area of intersection = width * height of the overlapping region\n","    intersect = intersect_w * intersect_h\n","\n","    # Calculate the area of each box\n","    w1, h1 = box1.xmax - box1.xmin, box1.ymax - box1.ymin  # Area of box1\n","    w2, h2 = box2.xmax - box2.xmin, box2.ymax - box2.ymin  # Area of box2\n","\n","    # Area of union = area of box1 + area of box2 - area of intersection\n","    union = w1 * h1 + w2 * h2 - intersect\n","\n","    # Return the Intersection over Union (IoU) which is the ratio of the intersection area to the union area\n","    return float(intersect) / union\n"],"metadata":{"id":"V_zid7O-EOGN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Non-Max Suppression\n","\n","It takes boxes that have the presence of objects in them along with non-max threshold as a parameter.\n","\n","The model has predicted a lot of candidate bounding boxes, and most of the boxes will be referring to the same objects. The list of bounding boxes can be filtered and those boxes that overlap and refer to the same object can be merged. We can define the amount of overlap as a configuration parameter, in this case, 50% or 0.5. This filtering of bounding box regions is generally referred to as non-maximal suppression and is a required post-processing step.\n","\n","Rather than purging the overlapping boxes, their predicted probability for their overlapping class is cleared. This allows the boxes to remain and be used if they also detect another object type.\n"],"metadata":{"id":"odCNBzb4mijW"}},{"cell_type":"code","source":["def do_nms(boxes, nms_thresh):\n","    # Check if the list of boxes is not empty\n","    if len(boxes) > 0:\n","        nb_class = len(boxes[0].classes)  # Get the number of classes based on the first box's class count\n","    else:\n","        return  # If no boxes are present, return early\n","\n","    # Iterate over each class to perform Non-Maximum Suppression (NMS)\n","    for c in range(nb_class):\n","        # Sort the boxes by their class confidence scores (in descending order)\n","        sorted_indices = np.argsort([-box.classes[c] for box in boxes])\n","\n","        # Iterate through the sorted list of boxes\n","        for i in range(len(sorted_indices)):\n","            index_i = sorted_indices[i]  # Get the index of the i-th box\n","\n","            # Skip the box if its confidence score for class 'c' is 0 (no object detected)\n","            if boxes[index_i].classes[c] == 0: continue\n","\n","            # Compare the current box with all remaining boxes\n","            for j in range(i+1, len(sorted_indices)):\n","                index_j = sorted_indices[j]  # Get the index of the j-th box\n","\n","                # If the IoU (Intersection over Union) of box i and box j is greater than the threshold, suppress the j-th box\n","                if bbox_iou(boxes[index_i], boxes[index_j]) >= nms_thresh:\n","                    boxes[index_j].classes[c] = 0  # Set the confidence score for class 'c' of box j to 0 (suppress the box)"],"metadata":{"id":"G2CBAU62mfWQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This will leave us with the same number of boxes, but only very few of interest. We can retrieve just those boxes that strongly predict the presence of an object: that is are more than 60% confident. This can be achieved by enumerating all boxes and checking the class prediction values. We can then look up the corresponding class label for the box and add it to the list. Each box must be considered for each class label, just in case the same box strongly predicts more than one object."],"metadata":{"id":"qBuq6qL3SMNR"}},{"cell_type":"markdown","source":["### Decode the output of the network:\n","\n","We will iterate through each of the NumPy arrays, one at a time, and decode the candidate bounding boxes and class predictions based on the object threshold.\n","\n","The first 4 elements will be the coordinates of the Bounding box, and 5th element will be the object score followed by the class probabilities."],"metadata":{"id":"UXKkwNPmSwaN"}},{"cell_type":"code","source":["def decode_netout(netout, anchors, obj_thresh, nms_thresh, net_h, net_w):\n","    # Extract the grid height and width from the shape of the network output\n","    grid_h, grid_w = netout.shape[:2]\n","\n","    nb_box = 3  # Number of bounding boxes per grid cell (common in YOLO)\n","\n","    # Reshape the output to separate bounding box attributes (x, y, w, h, objectness, and class probabilities)\n","    netout = netout.reshape((grid_h, grid_w, nb_box, -1))\n","\n","    # The number of classes is the total number of output elements minus 5 (4 for bounding box + 1 for objectness)\n","    nb_class = netout.shape[-1] - 5\n","\n","    # Initialize an empty list to store bounding boxes\n","    boxes = []\n","\n","    # Apply sigmoid function to the first two elements (x, y) and the last elements (objectness, class probabilities)\n","    netout[..., :2]  = _sigmoid(netout[..., :2])  # Sigmoid for x and y (center coordinates)\n","    netout[..., 4:]  = _sigmoid(netout[..., 4:])  # Sigmoid for objectness and class probabilities\n","\n","    # Calculate the confidence scores for the classes (objectness * class probabilities)\n","    netout[..., 5:]  = netout[..., 4][..., np.newaxis] * netout[..., 5:]\n","\n","    # Set the class probabilities to zero if they are below the objectness threshold\n","    netout[..., 5:] *= netout[..., 5:] > obj_thresh\n","\n","    # Iterate through all grid cells to decode bounding boxes\n","    for i in range(grid_h * grid_w):\n","        row = i // grid_w  # Calculate the row index of the current grid cell\n","        col = i % grid_w   # Calculate the column index of the current grid cell\n","\n","        # Iterate through all bounding boxes in the current grid cell\n","        for b in range(nb_box):\n","            # Get the objectness score (the 5th element in the output for each box)\n","            objectness = netout[int(row)][int(col)][b][4]\n","\n","            # Skip the box if its objectness score is less than the threshold\n","            if objectness.all() <= obj_thresh:\n","                continue\n","\n","            # Decode the bounding box coordinates (x, y, w, h)\n","            x, y, w, h = netout[int(row)][int(col)][b][:4]\n","\n","            # Convert the center coordinates (x, y) to the corresponding image dimensions\n","            x = (col + x) / grid_w  # x is normalized to the image width\n","            y = (row + y) / grid_h  # y is normalized to the image height\n","\n","            # Convert width and height to actual image dimensions using anchors\n","            w = anchors[2 * b + 0] * np.exp(w) / net_w  # width is adjusted using the corresponding anchor\n","            h = anchors[2 * b + 1] * np.exp(h) / net_h  # height is adjusted using the corresponding anchor\n","\n","            # Extract the class probabilities for the bounding box\n","            classes = netout[int(row)][col][b][5:]\n","\n","            # Create a BoundBox object for the detected bounding box and append it to the list\n","            box = BoundBox(x - w / 2, y - h / 2, x + w / 2, y + h / 2, objectness, classes)\n","            boxes.append(box)\n","\n","    return boxes"],"metadata":{"id":"DXEcrodE9IkS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Correcting the Yolo boxes.\n","\n","We have the bounding boxes but they need to be stretched back into the shape of the original image. This will allow plotting the original image and drawing the bounding boxes, detecting real objects.\n","\n"],"metadata":{"id":"6PwW63fLSXV6"}},{"cell_type":"code","source":["# Function to adjust the bounding box sizes according to the original image size\n","def correct_yolo_boxes(boxes, image_h, image_w, net_h, net_w):\n","    # Calculate the scaling factor to preserve the aspect ratio when resizing the image\n","    if (float(net_w) / image_w) < (float(net_h) / image_h):\n","        # If the network width-to-height ratio is smaller than the image's width-to-height ratio\n","        new_w = net_w\n","        new_h = (image_h * net_w) / image_w  # Scale the height proportionally to width\n","    else:\n","        # If the network height-to-width ratio is smaller than the image's height-to-width ratio\n","        new_h = net_w\n","        new_w = (image_w * net_h) / image_h  # Scale the width proportionally to height\n","\n","    # Correct the bounding boxes to the original image dimensions\n","    for i in range(len(boxes)):\n","        # Calculate the offset and scale for the x and y axes based on the resizing\n","        x_offset, x_scale = (net_w - new_w) / 2. / net_w, float(new_w) / net_w\n","        y_offset, y_scale = (net_h - new_h) / 2. / net_h, float(new_h) / net_h\n","\n","        # Adjust the bounding box coordinates based on the scaling factors and offset\n","        # The coordinates are scaled back to the original image size (image_w, image_h)\n","        boxes[i].xmin = int((boxes[i].xmin - x_offset) / x_scale * image_w)\n","        boxes[i].xmax = int((boxes[i].xmax - x_offset) / x_scale * image_w)\n","        boxes[i].ymin = int((boxes[i].ymin - y_offset) / y_scale * image_h)\n","        boxes[i].ymax = int((boxes[i].ymax - y_offset) / y_scale * image_h)"],"metadata":{"id":"vnoc5t8TmaIR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Drawing a white box around the object present in the image\n","      \n","The below function is implemented by taking the filename of the original. photograph and the parallel lists of bounding boxes, labels, and scores, and create a plot showing all detected objects."],"metadata":{"id":"WxeHAbLXXT9N"}},{"cell_type":"code","source":["# Function to draw bounding boxes on the image and display the class labels\n","def draw_boxes(image, boxes, labels, obj_thresh):\n","    for box in boxes:\n","        label_str = ''  # Initialize the label string for each box\n","        label = -1      # Initialize label index as -1 to signify no label initially\n","\n","        # Iterate over all class labels and check if the class score exceeds the threshold\n","        for i in range(len(labels)):\n","            if box.classes[i] > obj_thresh:\n","                label_str += labels[i]  # Append the class label to the label string\n","                label = i                # Update the label index to the current class\n","                print(labels[i] + ': ' + str(box.classes[i] * 100) + '%')  # Print class and its confidence\n","\n","        # If a valid label was found (score > threshold), draw a rectangle and label on the image\n","        if label >= 0:\n","            # Draw a rectangle around the detected object\n","            cv2.rectangle(image, (box.xmin, box.ymin), (box.xmax, box.ymax), (0, 255, 0), 3)\n","\n","            # Add the label text above the bounding box with the detection score\n","            cv2.putText(image,\n","                        label_str + ' ' + str(box.get_score()),  # Display label and score\n","                        (box.xmin, box.ymin - 13),              # Position the text above the box\n","                        cv2.FONT_HERSHEY_SIMPLEX,               # Use a simple font\n","                        1e-3 * image.shape[0],                   # Scale the font size\n","                        (0, 255, 0),                            # Green color for the text\n","                        2)                                      # Text thickness\n","\n","    return image"],"metadata":{"id":"BU7KptXOXRSO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Getting bounding box and prediction\n","\n","Now, we are going to use all the above functions to get the final output as object detection from the **y_pred**, that we got from the YOLO_v3 model initially by passing an image.\n","\n","#### Setting some parameters:"],"metadata":{"id":"toSFAr81UfVJ"}},{"cell_type":"code","source":["# Input image size for Yolov3 is 416 x 416 which we set using net_h and net_w.\n","net_h, net_w = 416, 416\n","\n","# Object threshold is set to 0.5 and Non-max suppression threshold is set to 0.45\n","obj_thresh, nms_thresh = 0.5, 0.45\n","\n","# We set the anchor boxes\n","# Anchor Boxes used to predict bounding boxes, YOLOv3 uses predefined bounding boxes\n","# called as anchors/priors and also these anchors/priors are used to calculate real width and real height for predicted bounding boxes.\n","anchors = [[116,90,  156,198,  373,326],  [30,61, 62,45,  59,119], [10,13,  16,30,  33,23]]\n","\n","# Then define the 80 labels for the Common Objects in Context (COCO) model to predict\n","# The following 80 classes are available using COCO’s pretrained weights:\n","labels = [\"person\", \"bicycle\", \"car\", \"motorbike\", \"aeroplane\", \"bus\", \"train\", \"truck\", \\\n","          \"boat\", \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \\\n","          \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\", \\\n","          \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\", \\\n","          \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\", \\\n","          \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \\\n","          \"apple\", \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \\\n","          \"chair\", \"sofa\", \"pottedplant\", \"bed\", \"diningtable\", \"toilet\", \"tvmonitor\", \"laptop\", \"mouse\", \\\n","          \"remote\", \"keyboard\", \"cell phone\", \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \\\n","          \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\", \"hair drier\", \"toothbrush\"]\n"],"metadata":{"id":"D4g24e0-MHOM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### We have already created a model and passed the image, now predicting the bounding box:"],"metadata":{"id":"QCeby0BsbjCT"}},{"cell_type":"code","source":["# summarize the shape of the list of arrays that we got from model as y_pred\n","print([a.shape for a in y_pred])"],"metadata":{"id":"6CCqp_WsOjca"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["boxes = []\n","\n","for i in range(len(y_pred)): # Iteration over the prediction list\n","    # decode the output of the network\n","    boxes += decode_netout(y_pred[i][0], anchors[i], obj_thresh, nms_thresh, net_h, net_w)\n","\n","# correct the sizes of the bounding boxes\n","correct_yolo_boxes(boxes, image_h, image_w, net_h, net_w)\n","\n","# suppress non-maximal boxes\n","do_nms(boxes, nms_thresh)\n","\n","# draw bounding boxes on the image using labels\n","draw_boxes(image, boxes, labels, obj_thresh)\n","\n","# write the image with bounding boxes to file\n","cv2.imwrite(image_path[:-4] + '_detected' + image_path[-4:], (image).astype('uint8'))"],"metadata":{"id":"g4o7k6UPOZzx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Plotting the Final Result\n","We plot the final output from the network. Please change the name of the detected image in order to plot your own result!"],"metadata":{"id":"xIcC5CSRM1Q5"}},{"cell_type":"code","source":["from google.colab.patches import cv2_imshow\n","\n","x = cv2.imread(\"bus_detected.jpg\")\n","#x = cv2.imread(\"dog_detected.jpg\")\n","\n","cv2_imshow(x)"],"metadata":{"id":"W69L63nmMzYN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### References\n","\n","1. [YOLOv3 Object Detection with Keras](https://drive.google.com/file/d/1ERd0CiKkB-KyhIoETQc1zS_AQzDkvd16/view?usp=sharing)\n","\n","2. [The beginner’s guide to implementing YOLOv3]( https://machinelearningspace.com/yolov3-tensorflow-2-part-1/#nms-unique)"],"metadata":{"id":"DrvWXviqZEDo"}},{"cell_type":"markdown","metadata":{"id":"VHfHdGCP_n6Y"},"source":["### Please answer the questions below to complete the experiment:\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VgSwVENIPcM6"},"outputs":[],"source":["#@title  Let us say, in a YOLO implementation, one image is divided into a 32x32 grid of cells and 3 bounding boxes. There are 10 classes of objects. What is the shape of the output tensor from YOLOv3?{run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n","Answer = \"\" #@param [\"\", \"(32,32,15)\", \"(32,32,30)\", \"(32,32,45)\", \"(15,32,45)\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NMzKSbLIgFzQ"},"outputs":[],"source":["#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n","Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DjcH1VWSFI2l"},"outputs":[],"source":["#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n","Additional = \"\" #@param {type:\"string\"}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4VBk_4VTAxCM"},"outputs":[],"source":["#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XH91cL1JWH7m"},"outputs":[],"source":["#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z8xLqj7VWIKW"},"outputs":[],"source":["#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"FzAZHt1zw-Y-","collapsed":true},"outputs":[],"source":["#@title Run this cell to submit your notebook for grading { vertical-output: true }\n","try:\n","  if submission_id:\n","      return_id = submit_notebook()\n","      if return_id : submission_id = return_id\n","  else:\n","      print(\"Please complete the setup first.\")\n","except NameError:\n","  print (\"Please complete the setup first.\")"]}]}