{"cells":[{"cell_type":"markdown","metadata":{"id":"2c2SSq-0fluo"},"source":["# Advanced Certification Programme in AI and MLOps\n","## A programme by IISc and TalentSprint\n","### Assignment 2: Leveraging a pre-trained model"]},{"cell_type":"markdown","metadata":{"id":"1IVfPFW_fwSY"},"source":["## Learning Objectives:\n","\n","At the end of the experiment, you will be able to:\n","\n","1. Understand and use a pre-trained model\n","2. Fine-tune the top layers while using a pre-trained model\n"]},{"cell_type":"markdown","metadata":{"id":"SsRvOU8bW3bj"},"source":["## Introduction\n","\n","A common and highly effective approach to deep learning on small image datasets is to use a pre-trained model.\n","\n","If the original dataset is large enough and general enough, the spatial hierarchy of features learned by the pre-trained model can effectively act as a generic model of the visual world, and hence, its features can prove useful for many different computer vision problems even though these new problems may involve completely different classes than those of the original task.\n","\n","There are two ways to use a pre-trained model:\n","  * feature extraction and\n","  * fine-tuning"]},{"cell_type":"markdown","metadata":{"id":"av8AYVUgXflR"},"source":["### Feature extraction"]},{"cell_type":"markdown","metadata":{"id":"bAe4IBm3YTDo"},"source":["A CNN typically consists of a:\n","* Convolutional base\n","* Densely connected classifier\n","\n","Key Idea -\n","* Features are learned by the convolutional base. So reuse it.\n","* Train a new classifier for your problem"]},{"cell_type":"markdown","metadata":{"id":"7NP5AuR09mow"},"source":["![picture](https://drive.google.com/uc?export=view&id=12FzJVGUCzQGDArYSqdABntKwKGHzIjVX)"]},{"cell_type":"markdown","metadata":{"id":"BNLA8HiKxQhc"},"source":["### Setup Steps:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2YzfoPvJDiTX"},"outputs":[],"source":["#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n","Id = \"\" #@param {type:\"string\"}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AjoZJWGErxGf"},"outputs":[],"source":["#@title Please enter your password (your registered phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n","password = \"\" #@param {type:\"string\"}"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"WBPPuGmBlDIN","colab":{"base_uri":"https://localhost:8080/","height":35},"collapsed":true,"outputId":"3b9fc930-f248-4283-da7e-0bcca873a0cd","executionInfo":{"status":"ok","timestamp":1728565115816,"user_tz":-330,"elapsed":739,"user":{"displayName":"Yograj Mahawar","userId":"10906189549763051896"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId=aimlops_c4&recordId=402\"></script>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Setup completed successfully\n"]}],"source":["#@title Run this cell to complete the setup for this Notebook\n","from IPython import get_ipython\n","\n","ipython = get_ipython()\n","\n","notebook= \"M2_AST_02_CNN_Transfer_Learning_C\" #name of the notebook\n","\n","def setup():\n","#  ipython.magic(\"sx pip3 install torch\")\n","\n","    # ipython.magic(\"wget https://cdn.iisc.talentsprint.com/AIandMLOps/Datasets/Acoustic_Extinguisher_Fire_Dataset.xlsx\")\n","    from IPython.display import HTML, display\n","    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n","    print(\"Setup completed successfully\")\n","    return\n","\n","def submit_notebook():\n","    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n","\n","    import requests, json, base64, datetime\n","\n","    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n","    if not submission_id:\n","      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","\n","      if r[\"status\"] == \"Success\":\n","          return r[\"record_id\"]\n","      elif \"err\" in r:\n","        print(r[\"err\"])\n","        return None\n","      else:\n","        print (\"Something is wrong, the notebook will not be submitted for grading\")\n","        return None\n","\n","    elif getAnswer() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n","      f = open(notebook + \".ipynb\", \"rb\")\n","      file_hash = base64.b64encode(f.read())\n","\n","      data = {\"complexity\" : Complexity, \"additional\" :Additional,\n","              \"concepts\" : Concepts, \"record_id\" : submission_id,\n","              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n","              \"notebook\" : notebook,\n","              \"feedback_experiments_input\" : Comments,\n","              \"feedback_mentor_support\": Mentor_support}\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","      if \"err\" in r:\n","        print(r[\"err\"])\n","        return None\n","      else:\n","        print(\"Your submission is successful.\")\n","        print(\"Ref Id:\", submission_id)\n","        print(\"Date of submission: \", r[\"date\"])\n","        print(\"Time of submission: \", r[\"time\"])\n","        print(\"View your submissions: https://aimlops-iisc.talentsprint.com/notebook_submissions\")\n","        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n","        return submission_id\n","    else: submission_id\n","\n","\n","def getAdditional():\n","  try:\n","    if not Additional:\n","      raise NameError\n","    else:\n","      return Additional\n","  except NameError:\n","    print (\"Please answer Additional Question\")\n","    return None\n","\n","def getComplexity():\n","  try:\n","    if not Complexity:\n","      raise NameError\n","    else:\n","      return Complexity\n","  except NameError:\n","    print (\"Please answer Complexity Question\")\n","    return None\n","\n","def getConcepts():\n","  try:\n","    if not Concepts:\n","      raise NameError\n","    else:\n","      return Concepts\n","  except NameError:\n","    print (\"Please answer Concepts Question\")\n","    return None\n","\n","\n","# def getWalkthrough():\n","#   try:\n","#     if not Walkthrough:\n","#       raise NameError\n","#     else:\n","#       return Walkthrough\n","#   except NameError:\n","#     print (\"Please answer Walkthrough Question\")\n","#     return None\n","\n","def getComments():\n","  try:\n","    if not Comments:\n","      raise NameError\n","    else:\n","      return Comments\n","  except NameError:\n","    print (\"Please answer Comments Question\")\n","    return None\n","\n","\n","def getMentorSupport():\n","  try:\n","    if not Mentor_support:\n","      raise NameError\n","    else:\n","      return Mentor_support\n","  except NameError:\n","    print (\"Please answer Mentor support Question\")\n","    return None\n","\n","def getAnswer():\n","  try:\n","    if not Answer:\n","      raise NameError\n","    else:\n","      return Answer\n","  except NameError:\n","    print (\"Please answer Question\")\n","    return None\n","\n","\n","def getId():\n","  try:\n","    return Id if Id else None\n","  except NameError:\n","    return None\n","\n","def getPassword():\n","  try:\n","    return password if password else None\n","  except NameError:\n","    return None\n","\n","submission_id = None\n","### Setup\n","if getPassword() and getId():\n","  submission_id = submit_notebook()\n","  if submission_id:\n","    setup()\n","else:\n","  print (\"Please complete Id and Password cells before running setup\")\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Hf1qet2EgGcL"},"source":["### Import libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sgPKI1GpnAn7"},"outputs":[],"source":["# Imports specific layers for building neural networks in Keras.\n","from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n","\n","# Imports the Input class, used to define the input shape for the model.\n","from tensorflow.keras import Input\n","\n","# Imports the Model class, which is used to create a Keras model.\n","from tensorflow.keras import Model\n","\n","# Imports the load_model function, which is used to load a saved model from disk.\n","from tensorflow.keras.models import load_model\n","\n","# Imports NumPy, a library for numerical and matrix operations, commonly used for handling and manipulating data arrays.\n","import numpy as np\n","\n","# Imports Pandas, a data manipulation library, especially useful for handling data in DataFrames.\n","import pandas as pd\n","\n","# Imports Matplotlib's Pyplot module, which is used for creating visualizations and plots.\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{"id":"seu6Ni2CX7CT"},"source":["Let us reuse the VGG16 network which has been trained on the ImageNet, which contains multiple classes of cats and dogs among other things.\n","\n","We expect the convolution base to have learned features that help it identify cats and dogs.\n","\n","Keras provides us with a pre-trained VGG16 network!\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fcNilE95iILQ"},"outputs":[],"source":["from tensorflow.keras.applications.vgg16 import VGG16"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6ExTNZyWnEx3"},"outputs":[],"source":["# Load the convolutional base of the pre-trained VGG16 model.\n","conv_base = VGG16(\n","    weights=\"imagenet\",          # Use pre-trained weights from the ImageNet dataset.\n","    include_top=False,           # Exclude the fully connected (dense) layers on top used for classification.\n","    input_shape=(180, 180, 3)    # Define the input shape as 180x180 pixels with 3 color channels (RGB).\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0b9kdoqLnPKx"},"outputs":[],"source":["# See a summary of the convolutional base\n","conv_base.summary()"]},{"cell_type":"markdown","metadata":{"id":"_UKbWJkUbMJt"},"source":["So how do we extract the features? Simple:\n","\n","Pass the images through the convolutional base."]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"S-Wl6uFLjL3Y"},"outputs":[],"source":["#@title Download the data\n","!wget -qq https://cdn.iisc.talentsprint.com/AIandMLOps/Datasets/cats_vs_dogs_small.zip\n","!unzip -qq '/content/cats_vs_dogs_small.zip'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rA46mBvMjORA"},"outputs":[],"source":["# Define the base directory for the dataset.\n","data_dir = '/content/cats_vs_dogs_small'\n","\n","# Define the path to the training data within the dataset directory.\n","train_path = data_dir + '/train'\n","\n","# Define the path to the validation data within the dataset directory.\n","validation_path = data_dir + '/validation'\n","\n","# Define the path to the test data within the dataset directory.\n","test_path = data_dir + '/test'"]},{"cell_type":"markdown","metadata":{"id":"K1JApk1OjbKR"},"source":["### Converting the image dataset into a workable format"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hjXVr-IhjWkB"},"outputs":[],"source":["# Import the function to load images from a directory and create a dataset.\n","from tensorflow.keras.utils import image_dataset_from_directory\n","\n","# Load the training dataset from the specified directory.\n","train_dataset = image_dataset_from_directory(\n","    train_path,                   # Path to the training data directory.\n","    image_size=(180, 180),        # Resize images to 180x180 pixels.\n","    batch_size=32                 # Define the number of images to include in each batch.\n",")\n","\n","# Load the validation dataset from the specified directory.\n","validation_dataset = image_dataset_from_directory(\n","    validation_path,              # Path to the validation data directory.\n","    image_size=(180, 180),        # Resize images to 180x180 pixels.\n","    batch_size=32                 # Define the batch size for validation.\n",")\n","\n","# Load the test dataset from the specified directory.\n","test_dataset = image_dataset_from_directory(\n","    test_path,                    # Path to the test data directory.\n","    image_size=(180, 180),        # Resize images to 180x180 pixels.\n","    batch_size=32                 # Define the batch size for testing.\n",")"]},{"cell_type":"markdown","metadata":{"id":"JQ0gpF96nqkT"},"source":["### Passing the dataset through Conv Base i.e forward pass through pre-trained weights\n","\n","Before forward pass, Preprocessing the dataset specific to VGG16 is also required."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LqSbNdyUkIRe"},"outputs":[],"source":["from tensorflow.keras.applications.vgg16 import preprocess_input       # Importing function for preprocessing specific to the vgg16"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gpcPOQXhU9tx"},"outputs":[],"source":["from tqdm import tqdm  # Import tqdm to display a progress bar for iterations. 'tqdm' means \"progress\" in Arabic.\n","\n","# Define a function to extract features and labels from a dataset using a pretrained model.\n","def get_features_and_labels(dataset):\n","    all_features = []  # Initialize an empty list to store features.\n","    all_labels = []    # Initialize an empty list to store labels.\n","\n","    # Loop through each batch of images and labels in the dataset, showing progress with tqdm.\n","    for images, labels in tqdm(dataset):\n","        preprocessed_images = preprocess_input(images)   # Preprocess images to fit the VGG16 input requirements.\n","        features = conv_base.predict(preprocessed_images, verbose=0)  # Forward pass to get features from VGG16.\n","\n","        # Append the extracted features and labels to their respective lists.\n","        all_features.append(features)\n","        all_labels.append(labels)\n","\n","    # Concatenate all features and labels into single NumPy arrays and return them.\n","    return np.concatenate(all_features), np.concatenate(all_labels)\n","\n","# Extract features and labels for each dataset using the get_features_and_labels function.\n","train_feature, train_labels = get_features_and_labels(train_dataset)\n","val_feature, val_labels = get_features_and_labels(validation_dataset)\n","test_feature, test_labels = get_features_and_labels(test_dataset)"]},{"cell_type":"markdown","metadata":{"id":"xyAs3DUVntAy"},"source":["### Defining and training the densely connected classifier\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KFHNlQaqnfo_"},"outputs":[],"source":["# Define a classifier model using Keras' Functional API.\n","inputs = Input(shape=(5,5,512))  # Input layer with a shape of (5, 5, 512), representing extracted features.\n","\n","# Flatten the input to transform the 5x5x512 feature map into a 1D vector.\n","x = Flatten()(inputs)\n","\n","# Add a fully connected (dense) layer with 256 units.\n","x = Dense(256)(x)\n","\n","# Add a dropout layer to reduce overfitting by randomly setting 50% of inputs to zero during training.\n","x = Dropout(0.5)(x)\n","\n","# Output layer with a single unit and a sigmoid activation function for binary classification.\n","outputs = Dense(1, activation=\"sigmoid\")(x)\n","\n","# Define the model by specifying the input and output layers.\n","model_without_conv_base = Model(inputs, outputs)\n","\n","# Compile the model with binary crossentropy loss for binary classification, rmsprop optimizer, and accuracy as the evaluation metric.\n","model_without_conv_base.compile(\n","    loss=\"binary_crossentropy\",\n","    optimizer=\"rmsprop\",\n","    metrics=[\"accuracy\"]\n",")"]},{"cell_type":"markdown","metadata":{"id":"gAY7NwrZM81q"},"source":["#### Call Back Function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eFC5KcwnsyuP"},"outputs":[],"source":["from tensorflow.keras.callbacks import ModelCheckpoint  # Import to save the model during training based on specific conditions.\n","from tensorflow.keras.callbacks import TensorBoard     # Import to enable logging for visualization in TensorBoard.\n","from tensorflow.keras.callbacks import EarlyStopping   # Import to stop training early if no improvement is detected."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pRjznrJmvYwE"},"outputs":[],"source":["# Define a function to return a list of commonly used callbacks.\n","def def_callbacks(filepath, mod_chk_mon=\"val_loss\", tensorboard=True, earlystop=0):\n","    callback_list = []  # Initialize an empty list to store callbacks.\n","\n","    # Default ModelCheckpoint callback to save the best model based on a monitored metric.\n","    callback_list.append(ModelCheckpoint(\n","        filepath + \".keras\",                # File path to save the model.\n","        save_best_only=True,     # Save only the best model (based on monitored metric).\n","        monitor=mod_chk_mon      # Metric to monitor (e.g., \"val_loss\" for validation loss).\n","    ))\n","\n","    # Optionally add a TensorBoard callback if tensorboard logging is enabled.\n","    if tensorboard:\n","        log_dir = \"tensorLog_\" + filepath  # Define log directory for TensorBoard.\n","        callback_list.append(TensorBoard(log_dir=log_dir))\n","\n","    # Optionally add an EarlyStopping callback if a patience parameter is provided.\n","    if earlystop > 0:\n","        callback_list.append(EarlyStopping(patience=earlystop))\n","\n","    return callback_list  # Return the list of callbacks."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_YHU9VyboGQs"},"outputs":[],"source":["# Fit (train) the densely connected classifier model.\n","history_wcb = model_without_conv_base.fit(\n","    train_feature, train_labels,             # Training data and labels.\n","    epochs=20,                               # Train for 20 epochs.\n","    validation_data=(val_feature, val_labels),  # Validation data and labels to monitor performance during training.\n","    callbacks=def_callbacks(\"feature_extraction_keras\")  # Callbacks to monitor and save model performance.\n",")"]},{"cell_type":"markdown","metadata":{"id":"oGFxlJVrqZZd"},"source":["Plotting Accuracy and Loss vs Epochs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AwU7lbh_oML5"},"outputs":[],"source":["import pandas as pd  # Import pandas for data handling.\n","data = pd.DataFrame(history_wcb.history)  # Convert the training history to a DataFrame.\n","epochs = range(1, len(data) + 1)  # Create a range for the number of epochs.\n","\n","# Plot training and validation accuracy vs. epochs.\n","plt.plot(epochs, data['accuracy'], 'bo', label=\"Training accuracy\")  # Plot training accuracy with blue dots.\n","plt.plot(epochs, data['val_accuracy'], 'b', label=\"Validation accuracy\")  # Plot validation accuracy with a blue line.\n","plt.legend()  # Display legend to distinguish between training and validation accuracy.\n","plt.xlabel(\"Epochs\")  # Label the x-axis as \"Epochs\".\n","plt.ylabel(\"Accuracy\")  # Label the y-axis as \"Accuracy\".\n","plt.title(\"Accuracy vs Epochs\")  # Set the plot title.\n","\n","# Plot training and validation loss vs. epochs.\n","plt.figure()  # Start a new figure for the loss plot.\n","plt.plot(epochs, data['loss'], 'bo', label=\"Training Loss\")  # Plot training loss with blue dots.\n","plt.plot(epochs, data['val_loss'], 'b', label=\"Validation Loss\")  # Plot validation loss with a blue line.\n","plt.legend()  # Display legend to distinguish between training and validation loss.\n","plt.xlabel(\"Epochs\")  # Label the x-axis as \"Epochs\".\n","plt.ylabel(\"Loss\")  # Label the y-axis as \"Loss\".\n","plt.title(\"Loss vs Epochs\")  # Set the plot title.\n","\n","plt.show()  # Display both plots."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8ohA0fCso6v3"},"outputs":[],"source":["# Load the pre-trained Keras model saved with the name \"feature_extraction_keras\"\n","test_model = load_model(\"feature_extraction_keras.keras\")\n","\n","# Evaluate the model on the test dataset (test_feature) and corresponding labels (test_labels)\n","# This function returns the loss value and the accuracy (or other metrics) of the model on the test data\n","test_loss, test_acc = test_model.evaluate(test_feature, test_labels)\n","\n","# Print the test accuracy, formatted to 3 decimal places\n","print(f\"Test accuracy:{test_acc:.3f}\")"]},{"cell_type":"markdown","metadata":{"id":"w5h2ONkvyZ2c"},"source":["The pretrained conv base + newly trained classification head has achieved roughly **97%** accuracy!\n","\n","Remember, for convnet_from_scratch_with_data_augmentation, it was ~ 85%\n","\n","For convnet_from_scratch, it was ~ 70%"]},{"cell_type":"markdown","metadata":{"id":"TpBJf_wrzSTf"},"source":["## Fine tuning"]},{"cell_type":"markdown","metadata":{"id":"0DrZOdGXzVBN"},"source":["The key idea here is to fine tune some top layers of the conv base as well.\n","\n","We do so by freezing most of the bottom layers, leaving only a few top layers to train. Lets see:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kTjJMKK2zr5_"},"outputs":[],"source":["# Load the convolutional base of the VGG16 model pre-trained on ImageNet\n","# 'weights=\"imagenet\"' loads the weights trained on the ImageNet dataset\n","# 'include_top=False' excludes the fully connected top layers (classifier), only keeping the convolutional base\n","# 'input_shape=(180,180,3)' specifies the input size for the model (180x180 images with 3 color channels)\n","conv_base = VGG16(\n","    weights=\"imagenet\",        # Use ImageNet pre-trained weights\n","    include_top=False,         # Exclude the top layers (classification layers) from the model\n","    input_shape=(180, 180, 3)  # Define the input shape as 180x180 RGB images\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B75ct3dqmibj"},"outputs":[],"source":["# Define the model's input layer with the shape of (180, 180, 3) representing 180x180 RGB images\n","inputs = Input(shape=(180, 180, 3))\n","\n","# Optionally, apply data augmentation to the input images (currently commented out)\n","# x = data_augmentation(inputs)\n","\n","# Preprocess the input images for the VGG16 model (this typically includes scaling pixel values)\n","x = preprocess_input(inputs)\n","\n","# Pass the input through the convolutional base (VGG16) to extract features\n","x = conv_base(x)\n","\n","# Flatten the output from the convolutional base (convert 2D feature maps to 1D)\n","x = Flatten()(x)\n","\n","# Add a fully connected (dense) layer with 256 units\n","x = Dense(256)(x)\n","\n","# Add dropout to the dense layer with a rate of 50% to reduce overfitting\n","x = Dropout(0.5)(x)\n","\n","# Add the output layer with a single unit and sigmoid activation for binary classification\n","outputs = Dense(1, activation=\"sigmoid\")(x)\n","\n","# Define the model with the input and output layers\n","model = Model(inputs, outputs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B9dnMykRt2vh"},"outputs":[],"source":["from tensorflow.keras.optimizers import RMSprop"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IxWDkFgyJx-e"},"outputs":[],"source":["conv_base.layers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NQ387fN1mooT"},"outputs":[],"source":["# Freeze all layers in the convolutional base up until the fourth layer from the last\n","# This will allow the last few layers to be fine-tuned during training while keeping the others fixed\n","conv_base.trainable = True  # Set all layers in the convolutional base as trainable initially\n","for layer in conv_base.layers[:-4]:   # Loop through all layers except the last 4\n","    layer.trainable = False  # Set each of these layers as non-trainable (they won't be updated during training)\n","\n","# Model compilation\n","# Set the loss function for binary classification (binary crossentropy)\n","# Use RMSprop optimizer with a very small learning rate (1e-5) to fine-tune the model without drastically changing the weights\n","model.compile(\n","    loss=\"binary_crossentropy\",                    # Loss function for binary classification\n","    optimizer=RMSprop(learning_rate=1e-5),         # Use a small learning rate to avoid drastic changes in pre-trained weights\n","    metrics=[\"accuracy\"]                           # Track accuracy during training\n",")\n","\n","# Display a summary of the model architecture (layers, number of parameters, etc.)\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BhI6SuYim0Hi"},"outputs":[],"source":["# Define a callback function to save the best model during training\n","# 'ModelCheckpoint' saves the model to the specified filepath only if the validation loss improves\n","callbacks = [ModelCheckpoint(filepath=\"fine_tuning_keras.keras\",  # Path where the best model will be saved\n","                              save_best_only=True,           # Only save the model if the validation loss improves\n","                              monitor=\"val_loss\")]           # Monitor the validation loss to determine improvement\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S_SJzWbGnQUc"},"outputs":[],"source":["## Training\n","history = model.fit(train_dataset, epochs=30, validation_data=validation_dataset, callbacks=callbacks)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F6L_SQlHnJj0"},"outputs":[],"source":["# Train the model using the fit() function\n","# 'train_dataset' is the training data, 'validation_dataset' is the validation data\n","# Training will run for 30 epochs, and the ModelCheckpoint callback will be used to save the best model\n","history = model.fit(\n","    train_dataset,                      # Training data (inputs and labels)\n","    epochs=30,                           # Number of epochs to train the model\n","    validation_data=validation_dataset,  # Validation data to monitor during training (used for validation loss and metrics)\n","    callbacks=callbacks                  # List of callback functions (e.g., ModelCheckpoint to save the best model)\n",")"]},{"cell_type":"markdown","metadata":{"id":"UBFUlWZwpMt5"},"source":["# Using already run and saved models\n","\n","For this save the model checkpoints in your drive (download the checkpoints and save to drive) give that path while loading.\n","\n","Mount your G drive:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tov-t4xmT_c6"},"outputs":[],"source":["# Mount Google Drive to access files stored in it\n","from google.colab import drive\n","drive.mount('/content/drive')  # This will prompt for authentication and mount your Google Drive at /content/drive\n","\n","# Change the current working directory to a specific folder in Google Drive\n","%cd \"/content/drive/My Drive/\"  # Change directory to the \"My Drive\" folder or any other folder in your Google Drive"]},{"cell_type":"markdown","metadata":{"id":"DC4PNMZGvOgo"},"source":["For example, you downloaded the `feature_extraction_keras` model and saved it in your drive inside a folder named 'model'. You can load it here like this:\n","\n","    model_saved_1 = load_model('/content/drive/MyDrive/model/feature_extraction_keras')"]},{"cell_type":"markdown","metadata":{"id":"N4fBwcIYudkJ"},"source":[" In this case the file path is simply : '/content/feature_extraction_keras'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zoNajzD7pntO"},"outputs":[],"source":["# Load the pre-trained model from the specified file path\n","model_fewa = load_model('/content/feature_extraction_keras.keras')  # Load the model saved at '/content/feature_extraction_keras'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2dky0dGDqoew"},"outputs":[],"source":["# Evaluate the loaded model on the test dataset (test_feature and test_labels)\n","# This returns two values: the loss and accuracy of the model on the test data\n","test_loss, test_acc = model_fewa.evaluate(test_feature, test_labels)\n","\n","# Print the test accuracy, formatted to 3 decimal places\n","print(f\"Test accuracy is:{test_acc:.3f}\")"]},{"cell_type":"markdown","metadata":{"id":"FzD_PIikOtew"},"source":["Great! feature_extraction model has achieved **97%** accuracy!"]},{"cell_type":"markdown","metadata":{"id":"VHfHdGCP_n6Y"},"source":["### Please answer the questions below to complete the experiment:\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VgSwVENIPcM6"},"outputs":[],"source":["#@title  What is the main idea of transfer learning in deep learning? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n","Answer = \"\" #@param [\"\", \"A) To train the pre-trained Convolutional Base (Conv_Base) from scratch on a new dataset or task.\", \"B) To use the pre-trained Convolutional Base (Conv_Base) as a feature extractor and add new layers to the top of the pre-trained model for fine-tuning on a new dataset or task.\", \"C) To use the pre-trained Convolutional Base (Conv_Base) as the only layers for the new dataset or task without any modifications.\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NMzKSbLIgFzQ"},"outputs":[],"source":["#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n","Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DjcH1VWSFI2l"},"outputs":[],"source":["#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n","Additional = \"\" #@param {type:\"string\"}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4VBk_4VTAxCM"},"outputs":[],"source":["#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XH91cL1JWH7m"},"outputs":[],"source":["#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z8xLqj7VWIKW"},"outputs":[],"source":["#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"FzAZHt1zw-Y-"},"outputs":[],"source":["#@title Run this cell to submit your notebook for grading { vertical-output: true }\n","try:\n","  if submission_id:\n","      return_id = submit_notebook()\n","      if return_id : submission_id = return_id\n","  else:\n","      print(\"Please complete the setup first.\")\n","except NameError:\n","  print (\"Please complete the setup first.\")"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}